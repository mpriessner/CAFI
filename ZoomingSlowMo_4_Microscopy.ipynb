{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ZoomingSlowMo_4_Microscopy.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2WnePAseIpyJ",
        "g37Oi5RM2yL2",
        "7c2dTKkx3J2_",
        "CC6RiGqaUvsh",
        "CIetBb0441xO",
        "IWFtMREDS_LB",
        "WT2RF0xWVWJm",
        "YAo1QSkzI8e2",
        "WHNHi--RJSik",
        "CTYX0W-4B2Fh",
        "smiapPDlJvgz",
        "mzn9aTH2LdU6",
        "mQc6KTA7K8be",
        "Az_PafRoyCos",
        "j_ogQXXbyJWo",
        "tta4phe6VMtO",
        "4_eedldMK8bs"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln3uO-uVz6ui"
      },
      "source": [
        "# **Zooming SlowMo 4 Microscopy**\n",
        "\n",
        "[**TUTORIAL Video **](https://youtu.be/xymw0ZRF8Xo): **WATCH THAT BEFORE USING IT**!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QDHAcGf1Yba"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<font size = 4> \"Zooming SlowMo\" is a network capable of interpolating images in series along the T and/or Z-dimension while simultaneously increasing the image resolution. This particular notebook allows the user to perform zooming-interpolation of **3D+t or 2D+t** microscopy images in **grayscale or RGB** which means it **improves the image resolution up to a factor of 8** and increasing the **frame frequency in T- and Z-dimension by a factor of 2**  \n",
        "\n",
        "---\n",
        "<font size = 4> e.g. Image input dimensions: (t,z,x,y) 10:15:256:256 -> Image output dimensions: 19:29:1024:1024 \n",
        "\n",
        "<font size = 4> or Image input dimensions: (t,z,x,y,c) 10:15:256:256:3 -> Image output dimensions: 19:29:1024:1024:3 \n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>This can be achieved by using a pretrained model or/and by fine-tuning our pretrained model with your data.\n",
        "\n",
        "<font size = 4> It was first published in the context of space-time video super-resolution in 2020 under the name \"Zooming-Slow-Mo: Fast and Accurate One-Stage Space-Time Video Super-Resolution\" by Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P. Allebach, Chenliang Xu in Institute of Electrical and Electronics Engineers (IEEE). \n",
        "\n",
        "[URL](https://openaccess.thecvf.com/content_CVPR_2020/html/Xiang_Zooming_Slow-Mo_Fast_and_Accurate_One-Stage_Space-Time_Video_Super-Resolution_CVPR_2020_paper.html) \n",
        "\n",
        "The network used in this application is a combination of a deformable ConvLSTM with a deep reconstruction network which achieves state of the art quantitative and qualitative performance in the zoom-interpolation task. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>*Disclaimer*:\n",
        "\n",
        "<font size = 4>This notebook is inspired from the *Zero-Cost Deep-Learning to Enhance Microscopy* project (ZeroCostDL4Mic) (https://github.com/HenriquesLab/DeepLearning_Collab/wiki) and was created by **Martin Priessner**\n",
        "\n",
        "<font size = 4>This notebook is based on the following paper: \n",
        "\n",
        "\n",
        "<font size = 4>The original source code found in: [here](https://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020) \n",
        "The modified version of the paper's source code adapted for this Google Colab is found in: [here](https://github.com/mpriessner/Zooming-Slow-Mo-CVPR-2020) \n",
        "\n",
        "\n",
        "<font size = 4>Example files for testing the algorithm can be found in the \"Demo\" folder in the downloaded repository after installing the dependencies.\n",
        "\n",
        "\n",
        "<font size = 4>**Please also cite this original paper when using or developing this notebook.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6ESGUX569x8"
      },
      "source": [
        "## **How to use this notebook?**\n",
        "\n",
        "---\n",
        "\n",
        "<font size = 4>Video describing how to use ZeroCostDL4Mic notebooks are available on youtube:\n",
        "  - [**Video 1**](https://www.youtube.com/watch?v=GzD2gamVNHI&feature=youtu.be): Full run through of the workflow to obtain the notebooks and the provided test datasets as well as a common use of the notebook\n",
        "  - [**Video 2**](https://www.youtube.com/watch?v=PUuQfP5SsqM&feature=youtu.be): Detailed description of the different sections of the notebook\n",
        "\n",
        "\n",
        "---\n",
        "###**Structure of a notebook**\n",
        "\n",
        "<font size = 4>The notebook contains two types of cell:  \n",
        "\n",
        "<font size = 4>**Text cells** provide information and can be modified by douple-clicking the cell. You are currently reading the text cell. You can create a new text by clicking `+ Text`.\n",
        "\n",
        "<font size = 4>**Code cells** contain code and the code can be modfied by selecting the cell. To execute the cell, move your cursor on the `[ ]`-mark on the left side of the cell (play button appears). Click to execute the cell. After execution is done the animation of play button stops. You can create a new coding cell by clicking `+ Code`.\n",
        "\n",
        "---\n",
        "###**Table of contents, Code snippets** and **Files**\n",
        "\n",
        "<font size = 4>On the top left side of the notebook you find three tabs which contain from top to bottom:\n",
        "\n",
        "<font size = 4>*Table of contents* = contains structure of the notebook. Click the content to move quickly between sections.\n",
        "\n",
        "<font size = 4>*Code snippets* = contain examples how to code certain tasks. You can ignore this when using this notebook.\n",
        "\n",
        "<font size = 4>*Files* = contain all available files. After mounting your google drive you will find your files and folders here. \n",
        "\n",
        "<font size = 4>**Remember that all uploaded files are purged after changing the runtime.** All files saved in Google Drive will remain. You do not need to use the Mount Drive-button; your Google Drive is connected in **Section 1.2**.\n",
        "\n",
        "<font size = 4>**Note:** The \"sample data\" in \"Files\" contains default files. Do not upload anything in here!\n",
        "\n",
        "---\n",
        "###**Making changes to the notebook**\n",
        "\n",
        "<font size = 4>**You can make a copy** of the notebook and save it to your Google Drive. To do this click file -> save a copy in drive.\n",
        "\n",
        "<font size = 4>To **edit a cell**, double click on the text. This will show you either the source code (in code cells) or the source text (in text cells).\n",
        "You can use the `#`-mark in code cells to comment out parts of the code. This allows you to keep the original code piece in the cell as a comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9BYbn3WDo5o"
      },
      "source": [
        "#**0.0 Before getting started**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgrOv8pdedup"
      },
      "source": [
        "---\n",
        "<font size = 4> This notebook provides the opportunity for you to either use a throurougly pretrined network to perform the task on your provided dataset or to finetune the pretrined network with your data. \n",
        "\n",
        "\n",
        "<font size = 4> The notebook may require a large amount of disk space. If training your own network with your own datasets, the available disk space on the user's google drive should contain at least 5-10GB.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYrsOhwJDwMg"
      },
      "source": [
        "---\n",
        "<font size = 4>**Data Format**\n",
        "\n",
        "<font size = 4> **The data used to train or test the ZS network can be the following: 3D stacks (TYX), 4D stacks (TZYX or TYXC) or 5D stacks (TZYXC) in .tiff (.tif) file format.** Furthermore the image need to be **8 bit**!!!\n",
        "\n",
        "\n",
        "To use this notebook on user data, upload the data in the your google drive or google cloud storage and execute the notebook cells step by step following the instructions in the associated section. \n",
        "\n",
        "<font size = 4> **Note: Your *dataset_folder* should not have spaces or brackets in its name as this is not recognized by the code and will throw an error** \n",
        "\n",
        "<font size = 4> - Training/Finetuning needs to be completet within 12h (for free Colab) 24h (for Colab Pro) - therefore it is recommended to do smaller training batches and keep continuing training the network. A copy of the pretrained network folder will be saved in a selected folder on your gdrive. \n",
        "\n",
        "\n",
        "<font size = 4>- In the folder `/content/ZoomInterpolation/demo/example` there is one 5D (TZYXC) file that you can use to discover the possibilities of this network.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<font size = 4>**Important notes**\n",
        "\n",
        "<font size = 4>- If when running the first cell the following error occurs \"not authored by google\" just press \"Run anyways\"\n",
        "\n",
        "<font size = 4>- If for some reason the notebook crashes and you cannot continue at all, you can first try to restart the runtime at `Runtime` -> `Restart Runtime`. This will delete all the variables and will reload all the libraries. (No re-install required). If that doesn't work you can perform a factory reset   `Runtime` -> `Factory reset runtime`. This will start a new virtual machine in the cloud and requires a new installation of the tool. \n",
        "\n",
        "<font size = 4>- If you wish to **Train or finetune a network** using your own dataset (and we encourage everyone to do that), you will need to run **Sections 4** \n",
        "\n",
        "<font size = 4>- If you plan to train or finetune the network on a very big dataset (more than 10 GB) we recommend to perform the training data preparation with offline on your PC running the python scrips provided in the following folder: `/content/ZoomInterpolation/codes/data_scripts/ZI_data_prep_scripts_modified` \n",
        "\n",
        "<font size = 4>- If you only wish to **run interpolation or/and resolution upscaling** using a pretrained model (provided in the pretrained folder or previously generated by you and saved on your Google Drive), you will need to run **Sections 2** to set up the notebook and then **Section 5** to perform the interpolation. \n",
        "\n",
        "<font size = 4>- If you want to **check the quality** of the network first run the **downsampling and/or downscaling option in section 3** and perform the image enhancement on the downsampled or low resolution image. The result can then be compared with the ground truth image in **Section 6**. \n",
        "\n",
        "<font size = 4>- When first connecting the notebook there will be warning saying \"not authored by google\" which can be ignored and just click \"connect\" to start with the notebook \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WnePAseIpyJ"
      },
      "source": [
        "# **1. Initialise the Colab session**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g37Oi5RM2yL2"
      },
      "source": [
        "\n",
        "### **1.1. Check for GPU access**\n",
        "---\n",
        "\n",
        "By default, the session should be using Python 3 and GPU acceleration, but it is possible to ensure that these are set properly by doing the following:\n",
        "\n",
        "<font size = 4>Go to **Runtime -> Change the Runtime type**\n",
        "\n",
        "<font size = 4>**Runtime type: Python 3** *(Python 3 is programming language in which this program is written)*\n",
        "\n",
        "<font size = 4>**Accelator: GPU** *(Graphics processing unit)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1trKgS0t7GnN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "eec8d69b-9324-4e01-a97d-85f59a986036"
      },
      "source": [
        "#@markdown ##Run this cell to check if you have GPU access\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name()=='':\n",
        "  print('You do not have GPU access.') \n",
        "  print('Did you change your runtime ?') \n",
        "  print('If the runtime settings are correct then Google did not allocate GPU to your session')\n",
        "  print('Expect slow performance. To access GPU try reconnecting later')\n",
        "\n",
        "else:\n",
        "  print('You have GPU access')\n",
        "\n",
        "from tensorflow.python.client import device_lib \n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "You have GPU access\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[name: \"/device:CPU:0\"\n",
              " device_type: \"CPU\"\n",
              " memory_limit: 268435456\n",
              " locality {\n",
              " }\n",
              " incarnation: 1182324088793529578, name: \"/device:XLA_CPU:0\"\n",
              " device_type: \"XLA_CPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 5389051417656246059\n",
              " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
              " device_type: \"XLA_GPU\"\n",
              " memory_limit: 17179869184\n",
              " locality {\n",
              " }\n",
              " incarnation: 11854733073591042312\n",
              " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
              " device_type: \"GPU\"\n",
              " memory_limit: 15964005991\n",
              " locality {\n",
              "   bus_id: 1\n",
              "   links {\n",
              "   }\n",
              " }\n",
              " incarnation: 8918508873839942789\n",
              " physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c2dTKkx3J2_"
      },
      "source": [
        "### **1.2. Mount your Google Drive or Google Cloud Bucket**\n",
        "---\n",
        "<font size = 4> To use this notebook on the data present in your Google Drive,  or your Google Cloud Bucket you need to mount your Google Drive or Google Cloud Bucket to this notebook.\n",
        "\n",
        "<font size = 4> Play one of the two the cells below to mount your Google Drive  or Google Cloud Bucket and follow the link. In the new browser window, select your drive and select 'Allow', copy the code, paste into the cell and press enter. This will give Colab access to the data on the drive. For Google Cloud Bucket you need to provide your bucket_name and project_id.\n",
        "\n",
        "<font size = 4> Once this is done, your data are available in the **Files** tab on the top left of notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKGzRiKtMCnc",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Run this cell to connect your Google Drive to Colab\n",
        "\n",
        "#@markdown * Click on the URL. \n",
        "\n",
        "#@markdown * Sign in your Google Account. \n",
        "\n",
        "#@markdown * Copy the authorization code. \n",
        "\n",
        "#@markdown * Enter the authorization code. \n",
        "\n",
        "#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\". \n",
        "\n",
        "#mounts user's Google Drive to Google Colab.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC6RiGqaUvsh"
      },
      "source": [
        "### Alternative for Google Drive - Google Cloud Storage (paid account necessary) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zeVvO3ScwRF",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Play the cell to connect your Google Cloud Bucket\n",
        "\n",
        "#@markdown * Click on the URL. \n",
        "\n",
        "#@markdown * Sign in your Google Account. \n",
        "\n",
        "#@markdown * Copy the authorization code. \n",
        "\n",
        "#@markdown * Enter the authorization code. \n",
        "\n",
        "#@markdown * Click on \"Files\" site on the right. Refresh the site. Your Google Drive folder should now be available here as \"drive\". \n",
        "\n",
        "# mount user's Cloud to Google Colab.\n",
        "import uuid\n",
        "from google.colab import auth\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "auth.authenticate_user()\n",
        "project_id = \"your-project_ID\" #@param {type:\"string\"}\n",
        "bucket_name = \"your_bucket_name\" #@param {type:\"string\"}\n",
        "bucket_name = bucket_name + str(uuid.uuid1())\n",
        "!gcloud config set project {project_id}\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse\n",
        "!mkdir Google_Cloud\n",
        "!gcsfuse --implicit-dirs martin_phd_project_data /content/Google_Cloud\n",
        "clear_output()\n",
        "\n",
        "#@markdown Before running the code make sure that you change the name of your bucket in the second last line of this cell: `!gcsfuse --implicit-dirs your_bucket_name /content/Google_Cloud`\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIetBb0441xO"
      },
      "source": [
        "\n",
        "# **2. Install ZoomInterpolation network and dependencies**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l0YL0MQWlWw",
        "cellView": "form"
      },
      "source": [
        "#@title ### Install the requirements \n",
        "#@markdown By running the cells the system requirements, libraries and the network are downloaded and installed .\n",
        "#@markdown This whole process may take 5-10 minutes and **will automatically restart the runtime** after finishing to load the necessary python modules correctly. \n",
        "#@markdown It already includes some basic demo files of simulated particles in the following folder `/content/ZoomInterpolation/demo`\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "!rm -rf \"/content/ZoomInterpolation\"\n",
        "!rm -rf \"/content/DCNv2.egg-info\"\n",
        "!rm -rf \"/content/build\"\n",
        "os.chdir(\"/content\")\n",
        "# !git clone --recursive https://github.com/mpriessner/Zooming-Slow-Mo-CVPR-2020.git ZoomInterpolation2\n",
        "!git clone https://github.com/mpriessner/CAFI.git\n",
        "shutil.move(\"/content/CAFI/ZS4Mic\", \"/content/ZoomInterpolation\")\n",
        "shutil.rmtree(\"/content/CAFI\")\n",
        "\n",
        "!pip install -r /content/ZoomInterpolation/requirements.txt\n",
        "\n",
        "!python /content/ZoomInterpolation/codes/models/modules/DCNv2/setup.py build develop\n",
        "\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "!pip install aicsimageprocessing\n",
        "!pip install tifffile \n",
        "!yes y | pip uninstall scipy\n",
        "!pip install scipy==1.1.0\n",
        "!pip install pympler\n",
        "!pip install aicsimageio==3.2.3\n",
        "\n",
        "# clear_output()\n",
        "\n",
        "\n",
        "# Download pretrained models\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0,'/content/ZoomInterpolation/load_functions')\n",
        "from download_from_gdrive import download_file_from_google_drive\n",
        "file_1x= \"16tsx1bKI8oRfp2eWGMgVkBwmoflQ81PT\"\n",
        "file_2x = \"178NfaFGm_T5FdNP1NXvjGDlRwjD7WA0C\"\n",
        "file_4x = \"1BoW2r5RVy34GcYjm5PYhizwtrt6npQk3\"\n",
        "pretrained_1x = \"/content/ZoomInterpolation/experiments/pretrained_models/pretrained_1x.pth\"\n",
        "pretrained_2x = \"/content/ZoomInterpolation/experiments/pretrained_models/pretrained_2x.pth\"\n",
        "pretrained_4x = \"/content/ZoomInterpolation/experiments/pretrained_models/pretrained_4x.pth\"\n",
        "download_file_from_google_drive(file_1x, pretrained_1x)\n",
        "download_file_from_google_drive(file_2x, pretrained_2x)\n",
        "download_file_from_google_drive(file_4x, pretrained_4x)\n",
        "\n",
        "\n",
        "# download demonstration files from simulated particle dataset\n",
        "# Download demo file\n",
        "path_example_GT = \"/content/ZoomInterpolation/demo/sim_particels_GT_512\"\n",
        "path_example_2DF_256 = \"/content/ZoomInterpolation/demo/sim_particles_2DF_256\"\n",
        "path_example_4DF_256 = \"/content/ZoomInterpolation/demo/sim_particles_2DF_2DF_256\"\n",
        "path_example_2DF_512 = \"/content/ZoomInterpolation/demo/sim_particles_2DF_512\"\n",
        "path_example_4DF_512 = \"/content/ZoomInterpolation/demo/sim_particles_2DF_2DF_512\"\n",
        "\n",
        "if not os.path.exists(\"/content/ZoomInterpolation/demo\"):\n",
        "  os.mkdir(\"/content/ZoomInterpolation/demo\")\n",
        "if not os.path.exists(path_example_GT):\n",
        "  os.mkdir(path_example_GT)\n",
        "if not os.path.exists(path_example_2DF_256):\n",
        "  os.mkdir(path_example_2DF_256)\n",
        "if not os.path.exists(path_example_4DF_256):\n",
        "  os.mkdir(path_example_4DF_256)\n",
        "if not os.path.exists(path_example_2DF_512):\n",
        "  os.mkdir(path_example_2DF_512)\n",
        "if not os.path.exists(path_example_4DF_512):\n",
        "  os.mkdir(path_example_4DF_512)\n",
        "\n",
        "\n",
        "example_GT = path_example_GT + \"/sim_particles_GT_512.tif\"\n",
        "example_2DF_256 = path_example_2DF_256 + \"/sim_particles_2DF_256.tif\"\n",
        "example_4DF_256 = path_example_4DF_256 + \"/sim_particle_2DF_2DF_256.tif\"\n",
        "example_2DF_512 = path_example_2DF_512 + \"/sim_particles_2DF_512.tif\"\n",
        "example_4DF_512 = path_example_4DF_512 + \"/sim_particles_2DF_2DF_512.tif\"\n",
        "\n",
        "demo_file_GT = \"1JQC78E0QBDbhiyNK-hPTYiiDWRkwfGd1\"\n",
        "demo_file_2DF_256 = \"1gKtp6y0aoYwMEefzC2oMJbXzfi3c_cCr\"\n",
        "demo_file_4DF_256 = \"1B1sls8n6irmjKkGNQculIe3M1ZeMEx2_\"\n",
        "demo_file_2DF_512 = \"1OPLXzCbTAviRVNUPYh7r67AKRYgSHmHn\"\n",
        "demo_file_4DF_512 = \"1CnMD97J-N9uP1letHwx7nMWgVCgroAdy\"\n",
        "\n",
        "\n",
        "download_file_from_google_drive(demo_file_GT, example_GT)\n",
        "download_file_from_google_drive(demo_file_2DF_256, example_2DF_256)\n",
        "download_file_from_google_drive(demo_file_4DF_256, example_4DF_256)\n",
        "download_file_from_google_drive(demo_file_2DF_512, example_2DF_512)\n",
        "download_file_from_google_drive(demo_file_4DF_512, example_4DF_512)\n",
        "\n",
        "\n",
        "\n",
        "# restart runtime\n",
        "os.kill(os.getpid(), 9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "RmdnUFsnS20L"
      },
      "source": [
        "#@title ### Run cell to avoid Google Colab disconnect while waiting\n",
        "import time\n",
        "time.sleep(4800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWFtMREDS_LB"
      },
      "source": [
        "##Download additional demo files (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PISP1qodS-UN",
        "cellView": "form"
      },
      "source": [
        "#@title ### Download additional demo files to play around with. After running this cell they will appear in the `/content/ZoomInterpolation/demo` folder.\n",
        "# Download demo file\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0,'/content/ZoomInterpolation/load_functions')\n",
        "from download_from_gdrive import download_file_from_google_drive\n",
        "path_example_256 = \"/content/ZoomInterpolation/demo/example_1_256_5D\"\n",
        "path_example_128 = \"/content/ZoomInterpolation/demo/example_1_128_5D\"\n",
        "\n",
        "if not os.path.exists(\"/content/ZoomInterpolation/demo\"):\n",
        "  os.mkdir(\"/content/ZoomInterpolation/demo\")\n",
        "if not os.path.exists(path_example_256):\n",
        "  os.mkdir(path_example_256)\n",
        "if not os.path.exists(path_example_128):\n",
        "  os.mkdir(path_example_128)\n",
        "\n",
        "example_file_256 = path_example_256 + \"/example_256.tif\"\n",
        "example_file_128 = path_example_128 + \"/example_128.tif\"\n",
        "demo_file_256 = \"1Wx_c05Z3IAZlCz5ybhiEO8zPWtr5BzGR\"\n",
        "demo_file_128 = \"1ECeMd3J4R30nKmDijWqYuPrWk0Cq_FIa\"\n",
        "download_file_from_google_drive(demo_file_256, example_file_256)\n",
        "download_file_from_google_drive(demo_file_128, example_file_128)\n",
        "\n",
        "\n",
        "os.mkdir(\"/content/ZoomInterpolation/demo/example_2_2048_3D\")\n",
        "os.mkdir(\"/content/ZoomInterpolation/demo/example_2_512_4D\")\n",
        "os.mkdir(\"/content/ZoomInterpolation/demo/example_2_256_4D\")\n",
        "os.mkdir(\"/content/ZoomInterpolation/demo/example_2_128_4D\")\n",
        "\n",
        "link_file = \"1CgbYzFXI8fnF5sFuuic6r6IzwOg4yqPi\"\n",
        "demo_flie_location = \"/content/ZoomInterpolation/demo/example_2_2048_3D/SHSY5Y_2048_3D.tif\"\n",
        "download_file_from_google_drive(link_file, demo_flie_location)\n",
        "\n",
        "file_512 = \"1FZ3G9dzDtsDGNPXe7sKDANuas4Tv_NI3\"\n",
        "demo_flie_location_512 = \"/content/ZoomInterpolation/demo/example_2_512_4D/SHSY5Y_512_4D.tif\"\n",
        "download_file_from_google_drive(file_512, demo_flie_location_512)\n",
        "\n",
        "file_256 = \"1mxR17ojfazlTAe4HfqrzT_bX0X_kwNwx\"\n",
        "demo_flie_location_256 = \"/content/ZoomInterpolation/demo/example_2_256_4D/SHSY5Y_256_4D.tif\"\n",
        "download_file_from_google_drive(file_256, demo_flie_location_256)\n",
        "\n",
        "file_128 = \"1_xMqFUkPq3J29i7tfTC6KeHvKVTB5mjL\"\n",
        "demo_flie_location_128 = \"/content/ZoomInterpolation/demo/example_2_128_4D/SHSY5Y_128_4D.tif\"\n",
        "download_file_from_google_drive(file_128, demo_flie_location_128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT2RF0xWVWJm"
      },
      "source": [
        "\n",
        "# **3. Image manipulation tools** (Optional - help to prepare your workflow)\n",
        "#####Not necessary for training part\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0AzGsX_6XWH"
      },
      "source": [
        "If you need to manipulate the images before performing training or testing you can do that here.\n",
        "Images for this tool need to be in TIF format.\n",
        "\n",
        "Options:\n",
        "- Check the dimensions of a TIF file\n",
        "- Downscale image lateral size\n",
        "- Upscale image lateral size\n",
        "- Downscale image frequency in t-dimension\n",
        "- Downscale image frequency in z-dimension "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N91WfL8TZaSe",
        "cellView": "form"
      },
      "source": [
        "#@title **Checking the dimensions of a selected TIF file**\n",
        "#@markdown Select a tif file to display the image dimensions.\n",
        "#@markdown The dimensions of your images should be one of the following:\n",
        "#@markdown ##**TZXYC** | **TZXY** | **TXY** | **ZXY**\n",
        "from skimage import io\n",
        "file_path = \"/content/ZoomInterpolation/demo/sim_particles_2DF_2DF_256/sim_particle_2DF_2DF_256.tif\"#@param {type:\"string\"}\n",
        "\n",
        "img = io.imread(file_path)\n",
        "print(f\"The TIF image dimensions are {str(img.shape)}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSC45hqUIC0k",
        "cellView": "form"
      },
      "source": [
        "#@title **Downscaling image resolution**\n",
        "#@markdown This cell scales the lateral image size down by a selected scale factor.\n",
        "\n",
        "#@markdown If your image has several channels tick the `with_channels` box.\n",
        "\n",
        "#@markdown The `source_path` field asks for the folder which contain your image sequences.\n",
        "\n",
        "\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "import cv2\n",
        "import os\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "\n",
        "down_scale_factor = 2#@param {type:\"number\"}\n",
        "ds_type = \"BIC\" #@param [\"BIC\", \"BIA\", .\"BIL\"] {type:\"string\"}\n",
        "\n",
        "source_path = \"/content/ZoomInterpolation/demo/sim_particles_2DF_2DF_512\"#@param {type:\"string\"}\n",
        "with_channels = False #@param{type:\"boolean\"}\n",
        "#use_RGB = False #@param{type:\"boolean\"}\n",
        "\n",
        "source_name = os.path.basename(source_path)\n",
        "save_parent = os.path.dirname(source_path)\n",
        "save_folder = source_name + \"_DS_\"+ str(down_scale_factor)+\"x\"\n",
        "save_path = os.path.join(save_parent, save_folder)\n",
        "if not os.path.exists(save_path):\n",
        "  os.mkdir(save_path)\n",
        "\n",
        "\n",
        "flist = os.listdir(source_path)\n",
        "flist = [f for f in flist if f.endswith(\"tif\")]\n",
        "for name in flist:\n",
        "  file_path = os.path.join(source_path, name)\n",
        "  img = io.imread(file_path)\n",
        "  # img = np.swapaxes(img, 0, 2)\n",
        "  print(file_path)\n",
        "  if with_channels == False:\n",
        "    if len(img.shape) == 4:\n",
        "      t, z,  y, x = img.shape\n",
        "    else:\n",
        "      t, y, x = img.shape \n",
        "      z = 1\n",
        "    img_temp = np.zeros((t, z, int(y/down_scale_factor), int(x/down_scale_factor)),dtype=np.uint8)\n",
        "    for i in range(t):\n",
        "      for j in range(z):\n",
        "        if len(img.shape) == 4:\n",
        "          img_slice = img[i,j, :, :]\n",
        "        else:\n",
        "          img_slice = img[i, :, :]\n",
        "        if ds_type == \"BIC\":\n",
        "            img_new = cv2.resize(img_slice, (int(y/down_scale_factor), int(x/down_scale_factor)), interpolation = cv2.INTER_CUBIC) # .INTER_CUBIC, .INTER_AREA, .INTER_LINEAR\n",
        "            str_type = \"IC\"\n",
        "        elif ds_type == \"BIA\":\n",
        "            img_new = cv2.resize(img_slice, (int(y/down_scale_factor), int(x/down_scale_factor)), interpolation = cv2.INTER_AREA) # .INTER_CUBIC, .INTER_AREA, .INTER_LINEAR\n",
        "            str_type = \"BIA\"\n",
        "        elif ds_type == \"BIL\":\n",
        "            img_new = cv2.resize(img_slice, (int(y/down_scale_factor), int(x/down_scale_factor)), interpolation = cv2.INTER_LINEAR) # .INTER_CUBIC, .INTER_AREA, .INTER_LINEAR\n",
        "            str_type = \"BIL\"\n",
        "        img_temp[i,j,: , :] = img_new\n",
        "  else:\n",
        "      if len(img.shape) == 5:\n",
        "         t, z,  y, x, c = img.shape\n",
        "      else:\n",
        "        t, y, x,c = img.shape \n",
        "        z = 1\n",
        "      img_temp = np.zeros((t, z, int(y/down_scale_factor), int(x/down_scale_factor),c), dtype=np.uint8)\n",
        "      for i in range(t):\n",
        "        for j in range(z):\n",
        "          if len(img.shape) == 5:\n",
        "             img_slice = img[i,j, :, :, :]\n",
        "          else:\n",
        "             img_slice = img[i, :, :, :]\n",
        "          if ds_type == \"BIC\":\n",
        "              img_new = cv2.resize(img_slice, (int(y/down_scale_factor), int(x/down_scale_factor)), interpolation = cv2.INTER_CUBIC) # .INTER_CUBIC, .INTER_AREA, .INTER_LINEAR\n",
        "              str_type = \"IC\"\n",
        "          elif ds_type == \"BIA\":\n",
        "              img_new = cv2.resize(img_slice, (int(y/down_scale_factor), int(x/down_scale_factor)), interpolation = cv2.INTER_AREA) # .INTER_CUBIC, .INTER_AREA, .INTER_LINEAR\n",
        "              str_type = \"BIA\"\n",
        "          elif ds_type == \"BIL\":\n",
        "              img_new = cv2.resize(img_slice, (int(y/down_scale_factor), int(x/down_scale_factor)), interpolation = cv2.INTER_LINEAR) # .INTER_CUBIC, .INTER_AREA, .INTER_LINEAR\n",
        "              str_type = \"BIL\"\n",
        "          img_temp[i,j,: , :, :] = img_new\n",
        "  # img_new = rescale(img, 2, anti_aliasing=False)\n",
        "  # img_new = resize(img, (33,1024,1024))\n",
        "  save_name = f\"DS_{str_type}_{down_scale_factor}x\" +  name\n",
        "  save_path_file = os.path.join(save_path, save_name)\n",
        "  io.imsave(save_path_file, img_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TvBpxnSjJIG",
        "cellView": "form"
      },
      "source": [
        "#@title **Upscaling lateral image size**\n",
        "\n",
        "#@markdown This cell scales up the lateral image size by a selected scale factor. \n",
        "\n",
        "#@markdown If your image has several channels tick the `with_channels` box.\n",
        "\n",
        "#@markdown The `source_path` field asks for the folder which contain your image sequences.\n",
        "\n",
        "\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "up_scale_factor = 2#@param {type:\"number\"}\n",
        "us_type = \"BIC\" #@param [\"BIC\", \"BIA\", \"BIL\"]\n",
        "\n",
        "source_path = \"/content/ZoomInterpolation/demo/sim_particles_2DF_2DF_512\"#@param {type:\"string\"}\n",
        "with_channels = False #@param{type:\"boolean\"}\n",
        "\n",
        "source_name = os.path.basename(source_path)\n",
        "save_parent = os.path.dirname(source_path)\n",
        "save_folder = source_name + \"_\"+ us_type+ str(up_scale_factor)+\"x\"\n",
        "save_path = os.path.join(save_parent, save_folder)\n",
        "if not os.path.exists(save_path):\n",
        "  os.mkdir(save_path)\n",
        "\n",
        "\n",
        "flist = os.listdir(source_path)\n",
        "flist = [f for f in flist if f.endswith(\"tif\")]\n",
        "if with_channels == False:\n",
        "  for name in flist:\n",
        "    file_path = os.path.join(source_path, name)\n",
        "    img = io.imread(file_path)\n",
        "    print(file_path)\n",
        "    if len(img.shape) == 4:\n",
        "      t, z,  y, x = img.shape\n",
        "    else:\n",
        "      t, y, x = img.shape \n",
        "      z = 1\n",
        "    img_temp = np.zeros((t, z, up_scale_factor*y, up_scale_factor*x),dtype=np.uint8)\n",
        "    for i in range(t):\n",
        "      for j in range(z):\n",
        "        if len(img.shape) == 4:\n",
        "          img_slice = img[i, j, :, :]\n",
        "\n",
        "        img_new = cv2.resize(img_slice, ( up_scale_factor*y, up_scale_factor*x), interpolation = cv2.INTER_CUBIC) # .INTER_CUBIC, .INTER_AREA, .INTER_LINEAR\n",
        "        img_temp[i,j,: , :] = img_new\n",
        "  save_name = f\"{us_type}_{up_scale_factor}x\" +  name\n",
        "  save_path_file = os.path.join(save_path, save_name)\n",
        "  io.imsave(save_path_file, img_temp)\n",
        "elif with_channels == True:\n",
        "  for name in flist:\n",
        "    file_path = os.path.join(source_path, name)\n",
        "    img = io.imread(file_path)\n",
        "    print(file_path)\n",
        "    if len(img.shape) == 5:\n",
        "      t, z,  y, x, c = img.shape\n",
        "    else:\n",
        "      t, y, x, c = img.shape \n",
        "      z = 1\n",
        "    img_temp = np.zeros((t,z, up_scale_factor*y, up_scale_factor*x,c),dtype=np.uint8)\n",
        "    for i in range(t):\n",
        "      for j in range(z):\n",
        "        if len(img.shape) == 5:\n",
        "          img_slice = img[i,j, :, :, :]\n",
        "        else:\n",
        "          img_slice = img[i, :, :, :]\n",
        "        img_new = cv2.resize(img_slice, ( up_scale_factor*y, up_scale_factor*x), interpolation = cv2.INTER_CUBIC) # .INTER_CUBIC, .INTER_AREA, .INTER_LINEAR\n",
        "        img_temp[i, j, : , :, :] = img_new\n",
        "  # img_new = rescale(img, 2, anti_aliasing=False)\n",
        "  # img_new = resize(img, (33,1024,1024))\n",
        "  save_name = f\"{us_type}_{up_scale_factor}x\" +  name\n",
        "  save_path_file = os.path.join(save_path, save_name)\n",
        "  io.imsave(save_path_file, img_temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mM4x8B1tB3BB",
        "cellView": "form"
      },
      "source": [
        "#@markdown ##Downscale image frequency\n",
        "#@markdown The `downscale_t` box lets you reduce the image number in temporal dimension.\n",
        "#@markdown The `downscale_t` box lets you reduce the image number in axial dimension.\n",
        "#@markdown The `source_path` field asks for the folder which contain your image sequences.\n",
        "\n",
        "\n",
        "downscale_t = True#@param{type:\"boolean\"}\n",
        "downscale_z = False#@param{type:\"boolean\"}\n",
        "\n",
        "#@markdown This options `limit_t_dim` allows the user limit the resulting image sequence to a selected `target_frame_number` (e.g., if `cut_target_frame_number` = 10 with `downscale_factor` = 2 and input dimensions = 25:512:512 in an TXY file, it will result in an image sequence of 10:512:512 dimensions)\n",
        "limit_t_dim = True#@param{type:\"boolean\"}\n",
        "\n",
        "cut_target_frame_number = 5#@param{type:\"number\"}\n",
        "\n",
        "def load_img(img_path):\n",
        "    img = io.imread(img_path)\n",
        "    img, use_RGB = correct_channels(img)\n",
        "    if img.shape[-1]==3:\n",
        "      use_RGB = True\n",
        "      t, z, y_dim, x_dim, _ = img.shape \n",
        "      print(\"This image will be processed as a RGB image\")\n",
        "    else:\n",
        "      use_RGB = False\n",
        "      t, z, y_dim, x_dim = img.shape \n",
        "    print(\"The image dimensions are: \" + str(img.shape))\n",
        "    return t, z, y_dim,x_dim, img, use_RGB\n",
        "\n",
        "\n",
        "def correct_channels(img):\n",
        "  '''For 2D + T (with or without RGB) a artificial z channel gets created'''\n",
        "  if img.shape[-1] ==3:\n",
        "    use_RGB = True\n",
        "  else:\n",
        "    use_RGB = False\n",
        "  if len(img.shape) ==4 and use_RGB:\n",
        "    t, x, y, c = img.shape\n",
        "    zeros = np.zeros((t,1,y,x,c),dtype=np.uint8)\n",
        "    zeros[:,0,:,:,:] = img\n",
        "    img = zeros\n",
        "  elif len(img.shape) ==3 and not use_RGB:\n",
        "    t, x, y = img.shape\n",
        "    zeros = np.zeros((t,1,y,x),dtype=np.uint8)\n",
        "    zeros[:,0,:,:] = img\n",
        "    img = zeros\n",
        "  return img, use_RGB\n",
        "\n",
        "#@title Downsampling of image\n",
        "# TODO- fix for RGB\n",
        "\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "downscale_factor =  2#@param {type:\"number\"}\n",
        "source_path = \"/content/ZoomInterpolation/demo/sim_particles_2DF_256\"#@param {type:\"string\"}\n",
        "source_name = os.path.basename(source_path)\n",
        "save_parent = os.path.dirname(source_path)\n",
        "save_folder = source_name + f\"_{downscale_factor}DF\"\n",
        "save_path = os.path.join(save_parent, save_folder)\n",
        "if not os.path.exists(save_path):\n",
        "  os.mkdir(save_path)\n",
        "\n",
        "\n",
        "\n",
        "flist = os.listdir(source_path)\n",
        "flist = [f for f in flist if f.endswith(\"tif\")]\n",
        "\n",
        "for name in flist:\n",
        "  file_path = os.path.join(source_path, name)\n",
        "  t, z, y_dim,x_dim, img, use_RGB = load_img(file_path)\n",
        "\n",
        "\n",
        "  if use_RGB == False:\n",
        "    if downscale_z and downscale_t:\n",
        "      label = \"_t_z_\"\n",
        "      img_new = img[::downscale_factor,::downscale_factor,:,:]\n",
        "    elif downscale_t:\n",
        "      label = \"_t_\"\n",
        "      img_new = img[::downscale_factor,:,:,:]\n",
        "    elif downscale_z:\n",
        "      label = \"_z_\"\n",
        "      img_new = img[:,::downscale_factor,:,:]\n",
        "  else:\n",
        "    if downscale_z and downscale_t:\n",
        "     label = \"_t_z_\"\n",
        "     img_new = img[::downscale_factor,::downscale_factor,:,:,:]\n",
        "    elif downscale_t:\n",
        "      label = \"_t_\"\n",
        "      img_new = img[::downscale_factor,:,:,:]\n",
        "    elif downscale_z:\n",
        "      label = \"_z_\"\n",
        "      img_new = img[:,::downscale_factor,:,:]\n",
        "\n",
        "  save_name = f\"DF_{downscale_factor}{label}\" +  name\n",
        "  save_path_file = os.path.join(save_path, save_name)\n",
        "  if use_RGB == False:\n",
        "    if limit_t_dim:\n",
        "      img_new = img_new[:cut_target_frame_number,:,:,:]\n",
        "  if use_RGB == True:\n",
        "    if limit_t_dim:\n",
        "      img_new = img_new[:cut_target_frame_number,:,:,:,:]\n",
        "  print(img_new.shape)\n",
        "  io.imsave(save_path_file, img_new)\n",
        "\n",
        "  # save_name = f\"DF2_DF_{downscale_factor}{label}\" +  name\n",
        "  # save_path_file = os.path.join(save_path, save_name)\n",
        "  # img_DS = img_new[::downscale_factor,::downscale_factor,:,:]\n",
        "  # print(img_DS.shape)\n",
        "\n",
        "  # io.imsave(save_path_file, img_DS)\n",
        "\n",
        "  # img_temp = convert(img_temp, 0, 255, np.uint8)\n",
        "  # io.imsave(\"DS-{}-{}.tif\".format(downscale_factor, name),img_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAo1QSkzI8e2"
      },
      "source": [
        "\n",
        "# **4. Training**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHNHi--RJSik"
      },
      "source": [
        "###**4.1 Prepare Data for Training**\n",
        "If you have small datasets for finetuning the network you can run the cells in this section to prepare the data. If you want to prepare bigger dataset (more than 5 GB) we recommend you to use the python scrips on your computer provided in the following folder: `/content/ZoomInterpolation/codes/data_scripts/ZI_data_prep_scripts_modified`\n",
        "You can upload the LMBD files to Google Colab via Google Drive or Google Cloud Storage and load it in the training stage. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Tcp6mY7JSix",
        "cellView": "form"
      },
      "source": [
        "#@title 4.1.1 Data preparation for training the network\n",
        "#@markdown by executing this cell the files will be prepared for training the network producing LMDB files.\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/content/ZoomInterpolation/load_functions')\n",
        "import os\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import cv2 \n",
        "import numpy as np\n",
        "from skimage import io\n",
        "import shutil\n",
        "import random\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "from preparation_for_training import get_all_filepaths_in_folder\n",
        "from preparation_for_training import get_img_dim\n",
        "from preparation_for_training import generate_mod_LR\n",
        "from prepare_for_zoominterpolation import correct_channels\n",
        "\n",
        "############ FIRST STEP ##################\n",
        "##### Separating the single images #######\n",
        "##########################################\n",
        "\n",
        "#@markdown Provide the folder with the training data\n",
        "# Define the necessary paths needed later\n",
        "Source_path = \"/content/ZoomInterpolation/demo/sim_particels_GT_512\"#@param {type:\"string\"}\n",
        "Parent_path = os.path.dirname(Source_path)\n",
        "test_train_seq_path = os.path.join(Parent_path, \"sequences\")\n",
        "\n",
        "# Paramenters\n",
        "test_train_split = 0.01 \n",
        "N_frames = 7 \n",
        "\n",
        "# create seq_lists *.txt\n",
        "train_seq_txt = os.path.join(Parent_path, \"sep_trainlist.txt\")\n",
        "test_seq_txt = os.path.join(Parent_path, \"sep_testlist.txt\")\n",
        "with open(train_seq_txt, \"w\") as f:\n",
        "  f.write(\"\")\n",
        "with open(test_seq_txt, \"w\") as f:\n",
        "  f.write(\"\")\n",
        "\n",
        "# delete test_train_folder if already exists\n",
        "if os.path.isdir(test_train_seq_path):\n",
        "  shutil.rmtree(test_train_seq_path)\n",
        "os.mkdir(test_train_seq_path)\n",
        "\n",
        "#get all files in the selected folder\n",
        "flist = get_all_filepaths_in_folder(Source_path)\n",
        "\n",
        "# split the different images and save them in the sequence folder with a given folderstructure\n",
        "# and create the test train split seq txt files\n",
        "for counter_1, file_path in tqdm(enumerate(flist)):\n",
        "  os.chdir(test_train_seq_path)\n",
        "  file_folder = \"%05d\"%(counter_1+1)\n",
        "  print(file_folder)\n",
        "  os.mkdir(file_folder)\n",
        "  file_folder_path = os.path.join(test_train_seq_path, file_folder)\n",
        "  os.chdir(file_folder_path)\n",
        "\n",
        "  img = io.imread(file_path)\n",
        "  # makes 3D into 4D dataset if needed\n",
        "  img, _ = correct_channels(img)\n",
        "  t_dim, z_dim, y_dim, x_dim, use_RGB = get_img_dim(img)\n",
        "\n",
        "  #calculate how many folders need to be created to cover all the images\n",
        "  N_folders_per_slice = math.ceil(t_dim/N_frames)\n",
        "  counter_2 = 1\n",
        "  for z in tqdm(range(z_dim)):\n",
        "    if (use_RGB and len(img.shape) ==5) or (use_RGB and len(img.shape) ==4):\n",
        "       img_slice = img[:, z, :, :, :]\n",
        "    else:\n",
        "      img_slice = img[:, z, :, :]\n",
        "   \n",
        "    for seq in range(1,N_folders_per_slice):\n",
        "      seq_folder = \"%04d\"%(counter_2)\n",
        "      seq_folder_path = os.path.join(file_folder_path, seq_folder)\n",
        "      os.mkdir(seq_folder_path)\n",
        "      counter_2 += 1\n",
        "      #create lever to randomly shift samples to test or train depending on the chosen split \n",
        "      test_train_lever = random.uniform(0, 1)\n",
        "      if test_train_lever < test_train_split:\n",
        "        with open(test_seq_txt, \"a\") as f:\n",
        "          f.write(f\"{file_folder}/{seq_folder}\\n\")\n",
        "      else:\n",
        "        with open(train_seq_txt, \"a\") as f:\n",
        "          f.write(f\"{file_folder}/{seq_folder}\\n\")\n",
        "\n",
        "      #save a given number of images as png in the new folder\n",
        "      for im_num in range(1, N_frames+1):\n",
        "        # print(((seq-1)*N_frames+(im_num-1)), y_dim, x_dim)\n",
        "        png_img_path = os.path.join(seq_folder_path, f\"im{im_num}.png\")\n",
        "        if use_RGB:\n",
        "            png_img = img_slice[((seq-1)*N_frames+(im_num-1)), :, :, :]\n",
        "            img_channels = png_img\n",
        "        else:\n",
        "            png_img = img_slice[((seq-1)*N_frames+(im_num-1)), :, :]\n",
        "            img_channels = np.zeros((y_dim, x_dim, 3))\n",
        "            img_channels[:,:,0] = png_img\n",
        "            img_channels[:,:,1] = png_img\n",
        "            img_channels[:,:,2] = png_img\n",
        "        io.imsave(png_img_path, img_channels)\n",
        "\n",
        "\n",
        "############ SECOND STEP ##################\n",
        "######## Generate HR LR images ############\n",
        "###########################################\n",
        "# This cell creates the following folder sequences in a provided output-path:\n",
        "# downscaled low-resolution (LR) \n",
        "# The original high-resolution (HR) \n",
        "\n",
        "# inPath = \"/\".join(folder_path.split(\"/\")[:-1]) \n",
        "##'/content/ZoomInterpolation/demo'#@param {type:\"string\"}\n",
        "sequences_path = os.path.join(Parent_path, \"sequences\")\n",
        "# test_or_train = \"test\"#@param [\"test\", \"train\"]\n",
        "\n",
        "outPath = os.path.join(Parent_path, \"Out_HR_LR\")\n",
        "scale_factor = 1 #@param [\"1\", \"2\", \"4\"] {type:\"raw\"}\n",
        "\n",
        "\n",
        "train_guide = os.path.join(Parent_path, \"sep_trainlist.txt\")\n",
        "test_guide = os.path.join(Parent_path, \"sep_testlist.txt\")\n",
        "\n",
        "N_frames = 7\n",
        "continue_loading = False\n",
        "log_path = os.path.join(outPath, \"HR_LR_log.txt\")\n",
        "save_HR, save_LR = generate_mod_LR(scale_factor, sequences_path, outPath, train_guide, test_guide,continue_loading, N_frames, log_path)\n",
        "\n",
        "\n",
        "############ THIRD STEP ###################\n",
        "#### Separate Test and Train samples ######\n",
        "###########################################\n",
        "# Split sequences folder into test and train data\n",
        "# The test data is not used in this notebook but can be used for quick training\n",
        "from preparation_for_training import run_split_sequence\n",
        "\n",
        "mode = \"HR\"\n",
        "test_or_train = \"test\"\n",
        "outPath_test = os.path.join(save_HR, f\"test_{scale_factor}\")\n",
        "sequences_path, train_guide, test_guide, outPath_test = run_split_sequence(mode, scale_factor, save_HR, save_LR, test_or_train, outPath_test)\n",
        "\n",
        "mode = \"HR\"\n",
        "test_or_train = \"train\"\n",
        "outPath_train = os.path.join(save_HR, f\"train_{scale_factor}\")\n",
        "sequences_path, train_guide, test_guide, outPath_test = run_split_sequence(mode, scale_factor, save_HR, save_LR, test_or_train, outPath_test)\n",
        "\n",
        "test_or_train = \"test\"\n",
        "mode = \"LR\"\n",
        "outPath_test = os.path.join(save_LR, f\"test_{scale_factor}\")\n",
        "sequences_path, train_guide, test_guide, outPath_test = run_split_sequence(mode, scale_factor, save_HR, save_LR, test_or_train, outPath_test)\n",
        "\n",
        "test_or_train = \"train\"\n",
        "mode = \"LR\"\n",
        "outPath_train = os.path.join(save_LR, f\"train_{scale_factor}\")\n",
        "sequences_path, train_guide, test_guide, outPath_test = run_split_sequence(mode, scale_factor, save_HR, save_LR, test_or_train, outPath_test)\n",
        "\n",
        "\n",
        "\n",
        "############ FOURTH STEP ###################\n",
        "#### Create LMDB Files for training ########\n",
        "############################################\n",
        "# Prepare lmdb folders for faster training \n",
        "# This step has to be performed with the HR **! and !** LR images to create the test data and train data folder\n",
        "#Select which data should be prepared into LMBD format\n",
        "'''create lmdb files for Vimeo90K-7 frames training dataset (multiprocessing)\n",
        "Will read all the images to the memory\n",
        "'''\n",
        "from preparation_for_training import prepare_lmbd\n",
        "\n",
        "HR_input_dim = 512#@param {type:\"integer\"}\n",
        "\n",
        "# if error occurs the batch value can be changed \n",
        "batch = 3000\n",
        "\n",
        "train_LMBD_HR, train_LMBD_LR, test_LMBD_HR, test_LMBD_LR, LR_input_dim  = prepare_lmbd(save_HR, save_LR, HR_input_dim, scale_factor, batch)\n",
        "\n",
        "shutil.rmtree(test_train_seq_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTYX0W-4B2Fh"
      },
      "source": [
        "###**4.2 Test if LMBD was produced correctly**\n",
        "Running this cell will create a few example files and will save them in `/content/\"number\".png`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxlGDnRrAE0u",
        "cellView": "form"
      },
      "source": [
        "import os,sys\n",
        "import os.path as osp\n",
        "import glob\n",
        "import pickle\n",
        "from multiprocessing import Pool\n",
        "import numpy as np\n",
        "import lmdb\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "select_folder = True #@param {type:\"boolean\"}\n",
        "if select_folder:\n",
        "  dataroot = '/content/ZoomInterpolation/demo/Out_HR_LR/HR/vimeo7_train_x1_HR.lmdb'#@param {type:\"string\"}\n",
        "else:\n",
        "  dataroot = train_LMBD_HR\n",
        "\n",
        "meta_info = pickle.load(open(osp.join(dataroot, 'Vimeo7_train_keys.pkl'), \"rb\"))\n",
        "print('Name: ', meta_info['name'])\n",
        "print('Resolution: ', meta_info['resolution'])\n",
        "print('# keys: ', len(meta_info['keys']))\n",
        "\n",
        "flist = meta_info['keys']\n",
        "\n",
        "env = lmdb.open(dataroot, readonly=True, lock=False, readahead=False, meminit=False)\n",
        "counter = 0\n",
        "for folder in flist:\n",
        "  if counter > 2:\n",
        "    break\n",
        "  else:\n",
        "    for i in range(1,8):\n",
        "      try:\n",
        "        key = folder + f\"_{i}\"\n",
        "        print(key)\n",
        "        with env.begin(write=False) as txn:\n",
        "          buf = txn.get(key.encode('ascii'))\n",
        "        img_flat = np.frombuffer(buf, dtype=np.uint8)\n",
        "        C, H, W = [int(s) for s in meta_info['resolution'].split('_')]\n",
        "        img = img_flat.reshape(H, W, C)\n",
        "        cv2.imwrite(f\"/content/{counter}.png\", img)\n",
        "        print(f\"Example images are saved in /content/{counter}.png\")\n",
        "      except:\n",
        "        key = folder + f\"_{i}\"\n",
        "        print(key)\n",
        "      counter +=1\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smiapPDlJvgz"
      },
      "source": [
        "### **4.3 Perform fine-tuning of pretrained network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3akt52CJvhB",
        "cellView": "form"
      },
      "source": [
        "#@title 4.3.1 Select data and parameters for training\n",
        "import os\n",
        "import os,sys\n",
        "sys.path.insert(0,'/content/ZoomInterpolation/load_functions')\n",
        "from preparation_for_training import change_train_yml\n",
        "from preparation_for_training import change_dataset_file\n",
        "from preparation_for_training import change_Sakuya_arch\n",
        "from preparation_for_training import change_train_file\n",
        "from prepare_split_images import bcolors\n",
        "\n",
        "# when trainining the network from scratch with the original 86gb Vimeo data\n",
        "original_trainingset = False\n",
        "\n",
        "#@markdown If you did not prepare the data in the step before you can load the data and set the parameters by checking the `select_folder_and_parameters` ticker. Select the folder created in the step before named \"vimeo7_test_x_HR.lmdb\" and \"vimeo7_test_x_LR.lmdb\". Make sure that you also select the scale factor and low resolution `LR_input_dim` and high resolution `HR_input_dim` images correctly. \n",
        "#@markdown The field `niter` is a parameter that determines how long the network will be trained.\n",
        "select_folder_and_parameters = False #@param {type:\"boolean\"}\n",
        "if select_folder_and_parameters:\n",
        "  train_LMBD_HR = '/content/ZoomInterpolation/demo/Out_HR_LR/HR/vimeo7_train_x1_HR.lmdb'#@param {type:\"string\"}\n",
        "  train_LMBD_LR = \"/content/ZoomInterpolation/demo/Out_HR_LR/HR/vimeo7_train_x1_HR.lmdb\"#@param {type:\"string\"}\n",
        "  cache_keys = os.path.join(train_LMBD_HR, \"Vimeo7_train_keys.pkl\")\n",
        "\n",
        "else:\n",
        "  cache_keys = os.path.join(train_LMBD_HR, \"Vimeo7_train_keys.pkl\")\n",
        "  \n",
        "scale_factor =  1#@param {type:\"number\"}\n",
        "LR_input_dim =  512#@param {type:\"number\"}\n",
        "HR_input_dim = 512 #@param {type:\"number\"}\n",
        "#@markdown In original code: 600000\n",
        "niter = 1000#@param {type:\"integer\"}\n",
        "#@markdown This will finetune the network for the selected scale factor\n",
        "finetune_pretrained_model = True #@param {type:\"boolean\"}\n",
        "if scale_factor == 2:\n",
        "  # pretrained_network_pth = \"/content/ZoomInterpolation/experiments/pretrained_models/pretrained_2x.pth\"\n",
        "  pretrained_network_pth = \"/content/ZoomInterpolation/experiments/pretrained_models/pretrained_2x.pth\"\n",
        "elif scale_factor == 4:\n",
        "  pretrained_network_pth = \"/content/ZoomInterpolation/experiments/pretrained_models/pretrained_4x.pth\"\n",
        "else:\n",
        "  pretrained_network_pth = \"/content/ZoomInterpolation/experiments/pretrained_models/pretrained_1x.pth\"\n",
        "\n",
        "#If no state file is existing just put \"~\"\n",
        "pretrained_network_state = \"~\"\n",
        "save_checkpoint_freq = \"!!float 1e2\"  #@param [\"!!float 5e3\", \"!!float 2e3\", \"!!float 1e3\", \"!!float 5e2\", \"!!float 1e2\"]\n",
        "#@markdown Select a location on your Gdrive where the model backup should be saved in between in case google colab resets.\n",
        "backup_location = \"/content/sample_data\"#@param {type:\"string\"}\n",
        "\n",
        "#@markdown **Advanced**:\n",
        "learning_rate  =  \"!!float 1e-5\" #@param [\"!!float 4e-4\", \"!!float 3e-4\", \"!!float 2e-4\",\"!!float 1e-4\", \"!!float 5e-5\", \"!!float 1e-5\", \"!!float 1e-6\"]\n",
        "debug = \"False\" # [\"True\",  \"False\"]\n",
        "\n",
        "#@markdown The number -1 means no warm-up. Any number >0 is warmup.\n",
        "warmup_iter = 4000 #@param {type:\"number\"}\n",
        "strict_load = \"true\"#[\"true\", \"false\"]\n",
        "LR_input_dim = int(LR_input_dim)\n",
        "\n",
        "# the following functions perform the changes of parameters in the files and require a restart runtime\n",
        "change_train_yml(train_LMBD_HR, train_LMBD_LR, scale_factor, cache_keys, niter,\\\n",
        "              finetune_pretrained_model, pretrained_network_pth,\\\n",
        "              pretrained_network_state, save_checkpoint_freq, warmup_iter, debug, learning_rate)\n",
        "\n",
        "change_dataset_file(HR_input_dim, LR_input_dim, scale_factor, original_trainingset)\n",
        "\n",
        "change_Sakuya_arch(scale_factor)\n",
        "\n",
        "change_train_file(backup_location)\n",
        "\n",
        "print(\"The files have been successfully updated. Please restart the runtime to load the changed settings!\")\n",
        "\n",
        "#@markdown A copy of the pretrained model folder will be saved every \"save_checkpoint_freq\" iterations in your selected \"backup_location\" folder. \n",
        "%reload_ext autoreload\n",
        "os.chdir(\"/content/ZoomInterpolation\")\n",
        "!python \"/content/ZoomInterpolation/codes/train.py\" -opt \"/content/ZoomInterpolation/codes/options/train/train_zsm.yml\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpaE_IX6lqEo",
        "cellView": "form"
      },
      "source": [
        "#@title 4.3.2 Update the trained model\n",
        "#@markdown Run this cell to replace the model with the new fine tuned model. Just the model with the upscale factor that you have benn training on will be updated. If you wish to also train the other upscale factore networks you need to do this seperately. \n",
        "\n",
        "model_folder = \"/content/ZoomInterpolation/experiments/LunaTokis_scratch_b16p32f5b40n7l1_600k_Vimeo/models\"\n",
        "last_trained_model = os.listdir(model_folder)[-1]\n",
        "last_model = os.path.join(model_folder,last_trained_model)\n",
        "replace_pretrained_model = \"/content/ZoomInterpolation/experiments/pretrained_models/pretrained_{}x.pth\".format(scale_factor)\n",
        "if os.path.exists(replace_pretrained_model):\n",
        "  os.remove(replace_pretrained_model)\n",
        "shutil.copy(last_model, replace_pretrained_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HQIcs6pJvhD",
        "cellView": "form"
      },
      "source": [
        "#@title 4.3.3 Visualize training results\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "#@markdown Either select the folder named \"LunaTokis...\" stored on your Google Drive then check `selected_train_folder`, otherwise the last performed training will be visualized\n",
        "select_train_folder = False #@param {type:\"boolean\"}\n",
        "\n",
        "if select_train_folder:\n",
        "  save_train_path = \"/content/ZoomInterpolation/experiments/LunaTokis_scratch_b16p32f5b40n7l1_600k_Vimeo\"#@param {type:\"string\"}\n",
        "else:\n",
        "  save_train_path = \"/content/ZoomInterpolation/experiments/LunaTokis_scratch_b16p32f5b40n7l1_600k_Vimeo\"\n",
        "\n",
        "# select the most recent trainings log file\n",
        "log_file_name = [i for i in os.listdir(save_train_path) if \"log\" in i][-1]\n",
        "log_file_path = os.path.join(save_train_path, log_file_name)\n",
        "train_list = []\n",
        "# save all the pixel error values in a list\n",
        "with open(log_file_path) as f:\n",
        "  for i in f:\n",
        "    if \"l_pix\" in i:\n",
        "       train_list.append(i[-12:])\n",
        "train_list = [float(i[:-3]) for i in train_list]\n",
        "\n",
        "num_items = len(train_list)\n",
        "x_axis = [i for i in range(0,num_items)]\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(x_axis, train_list)\n",
        "plt.title(\"Training progress\")\n",
        "plt.xlabel(\"Iter\")\n",
        "plt.ylabel(\"Pixel error\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzn9aTH2LdU6"
      },
      "source": [
        "\n",
        "# **5. Perform Interpolation and/or Lateral Image upscaling**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G46fUTz8K56u"
      },
      "source": [
        "After training or fine tuning of the network is finished you can just select the folder with images which you want to interpolate and upscale and check the dimensions that you want to improve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQc6KTA7K8be"
      },
      "source": [
        "### **5.1 Perform ZoomInterpolation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az_PafRoyCos"
      },
      "source": [
        "#### Select pretrained model from drive to use for network (optional)\n",
        "Otherwise our provided pretained networks will be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvz5KpRaTUG1",
        "cellView": "form"
      },
      "source": [
        "#@title  Select pretrained model from drive to use for network\n",
        "#@markdown Run this cell to replace your previously fine-tuned network model with the right zoomfactor to be used for the network. \n",
        "import os\n",
        "import shutil\n",
        "\n",
        "select_model = \"/content/ZoomInterpolation/experiments/LunaTokis_scratch_b16p32f5b40n7l1_600k_Vimeo/models/100_G.pth\"#@param {type:\"string\"}\n",
        "#@markdown Select the zoomfactor model that you want to replaced.\n",
        "zoomfactor = 1 #@param [\"1\", \"2\", \"4\"] {type:\"raw\"}\n",
        "\n",
        "path_1x = \"/content/ZoomInterpolation/experiments/pretrained_models/pretrained_1x.pth\"\n",
        "path_2x = \"/content/ZoomInterpolation/experiments/pretrained_models/pretrained_2x.pth\"\n",
        "path_4x = \"/content/ZoomInterpolation/experiments/pretrained_models/pretrained_4x.pth\"\n",
        "if zoomfactor ==1:\n",
        "    os.remove(path_1x)\n",
        "    shutil.copy(select_model, path_1x)\n",
        "elif zoomfactor ==2:\n",
        "    os.remove(path_2x)\n",
        "    shutil.copy(select_model, path_2x)\n",
        "elif zoomfactor ==2:\n",
        "    os.remove(path_4x)\n",
        "    shutil.copy(select_model, path_4x) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_ogQXXbyJWo"
      },
      "source": [
        "####Run Interpolation or/and resolution upscaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffBjXm2lIn2-",
        "cellView": "form"
      },
      "source": [
        "#@markdown #Run Interpolation or/and resolution upscaling\n",
        "############ SELECT OPTIONS ################\n",
        "############################################\n",
        "############################################\n",
        "##@markdown Large images bigger than 512px are split to smaller sizes that can be set by determining the divisor.\n",
        "\n",
        "# Customized functions for image preparation: splitting, augmentation\n",
        "import sys\n",
        "sys.path.insert(0,'/content/ZoomInterpolation/load_functions')\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from skimage import io\n",
        "from tqdm import tqdm\n",
        "from prepare_split_images import bcolors\n",
        "from prepare_split_images import img_split_pipeline\n",
        "from prepare_for_zoominterpolation import get_zoomfactor\n",
        "from prepare_for_zoominterpolation import data_preparation_for_zoominterpolation\n",
        "from prepare_for_zoominterpolation import prepare_files_for_zoominterpolation_step\n",
        "\n",
        "#@markdown **Select the folder where the images are located**\n",
        "Source_path = \"/content/ZoomInterpolation/demo/sim_particles_2DF_2DF_256\" #@param {type:\"string\"}\n",
        "Saving_path = \"/\".join(Source_path.split(\"/\")[:-1])\n",
        "\n",
        "#@markdown Provide the size of the images\n",
        "img_size = 256 #@param {type:\"integer\"}\n",
        "if img_size<=512:\n",
        "  divisor = img_size\n",
        "else: \n",
        "  divisor = 512\n",
        "\n",
        "\n",
        "# Choose option for zoom or and interpolation with or without downsampling\n",
        "# folder_option = \"upsample-t\" #@param [\"downsample-t\", \"downsample-z\", \"upsample-t\", \"upsample-z\", \"zoom\"]\n",
        "perform_t_interpolation = True #@param {type:\"boolean\"}\n",
        "perform_z_interpolation = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "zoomfactor = 1 #@param [\"1\", \"2\", \"4\", \"8\"] {type:\"raw\"}\n",
        "zoomfactor_1, zoomfactor_2 = get_zoomfactor(zoomfactor)\n",
        "\n",
        "remove_folder = []\n",
        "#######################\n",
        "### T-interpolation ###\n",
        "if perform_t_interpolation == True: \n",
        "  folder_option_1 = \"upsample-t\"\n",
        "\n",
        "  # function splits the image in smaller tiles if it is bigger than 512 due to RAM limitations - will be reconstructed later\n",
        "  aug_saving_path, split_img_folder_path,  log_path_file, multiplyer, nr_z_slices, nr_channels, nr_timepoints, x_dim, y_dim, x_div, y_div, use_RGB = img_split_pipeline(Source_path, Saving_path, divisor)\n",
        "\n",
        "  # create new folder for image separation\n",
        "  save_location = \"/\".join(split_img_folder_path.split(\"/\")[:-1])\n",
        "  folder_name = f\"{divisor}_img_separation\"\n",
        "  save_location = os.path.join(save_location,folder_name)\n",
        "  if not os.path.exists(save_location):\n",
        "    os.mkdir(save_location)\n",
        "  os.chdir(save_location)  \n",
        "\n",
        "  # Files are split into separate folder\n",
        "  sub_save_location = data_preparation_for_zoominterpolation(folder_option_1, save_location, split_img_folder_path)\n",
        "\n",
        "\n",
        "  # this function changes several files in the repo to fit the right settings such as resolution scale factor\n",
        "  img_folder_path_interpolate = prepare_files_for_zoominterpolation_step(sub_save_location, 1) # last option is zoom factor\n",
        "\n",
        "# OLD - ignor blur correction\n",
        "  # #@markdown Under certain circumstances the interpolated image is brighter and pixelated compared to the normal zoomed ones. Then this option helps to reduce the differences by blurring it with a 3x3 kernal and ajusting the gamma for the images to make them more similar.\n",
        "  blur_and_brightness_correction = False ##param{type:\"boolean\"}\n",
        "  os.chdir(\"/content/ZoomInterpolation/codes\")\n",
        "  !python test_new.py  --corr $blur_and_brightness_correction\n",
        "\n",
        "  # rename the folder that was created with date at the beginning\n",
        "  today = datetime.now()\n",
        "  date_info = today.strftime('%Y%m%d_%H%M%S')\n",
        "  interpolate_location = \"/content/ZoomInterpolation/results/Custom_\"+date_info \n",
        "  os.rename(\"/content/ZoomInterpolation/results/Custom\", interpolate_location)\n",
        "\n",
        "\n",
        "  import sys\n",
        "  import os\n",
        "  sys.path.insert(0,'/content/ZoomInterpolation/load_functions')\n",
        "  from prepare_for_zoominterpolation import save_interpolated_image\n",
        "\n",
        "  Source_path = save_interpolated_image(interpolate_location, Saving_path, log_path_file, divisor, 1, folder_option_1, use_RGB) # zoomfactor 1\n",
        "  remove_folder.append(Source_path)\n",
        "\n",
        "#######################\n",
        "### z-interpolation ###\n",
        "\n",
        "if perform_z_interpolation == True:\n",
        "  folder_option_2 = \"upsample-z\"\n",
        "\n",
        "  Saving_path = \"/\".join(Source_path.split(\"/\")[:-1])\n",
        "\n",
        "  # performs the splitting and datapreparation pipeline\n",
        "  aug_saving_path, split_img_folder_path,  log_path_file, multiplyer, nr_z_slices, nr_channels, nr_timepoints, x_dim, y_dim, x_div, y_div, use_RGB = img_split_pipeline(Source_path, Saving_path, divisor)\n",
        "\n",
        "  # create new folder for image separation\n",
        "  save_location = \"/\".join(split_img_folder_path.split(\"/\")[:-1])\n",
        "  folder_name = f\"{divisor}_img_separation\"\n",
        "  save_location = os.path.join(save_location,folder_name)\n",
        "  if not os.path.exists(save_location):\n",
        "    os.mkdir(save_location)\n",
        "  os.chdir(save_location)  \n",
        "\n",
        "  sub_save_location = data_preparation_for_zoominterpolation(folder_option_2, save_location, split_img_folder_path)\n",
        "  \n",
        "  img_folder_path_interpolate = prepare_files_for_zoominterpolation_step(sub_save_location, 1)\n",
        " \n",
        "  %reload_ext autoreload\n",
        "  os.chdir(\"/content/ZoomInterpolation/codes\")\n",
        "  !python test_new.py  --corr $blur_and_brightness_correction\n",
        "\n",
        "\n",
        "  today = datetime.now()\n",
        "  date_info = today.strftime('%Y%m%d_%H%M%S')\n",
        "  interpolate_location = \"/content/ZoomInterpolation/results/Custom_\"+date_info \n",
        "  os.rename(\"/content/ZoomInterpolation/results/Custom\", interpolate_location)\n",
        "\n",
        "  Source_path = save_interpolated_image(interpolate_location, Saving_path, log_path_file, divisor, 1, folder_option_2, use_RGB) # zoomfactor choses as 1\n",
        "  remove_folder.append(Source_path)\n",
        "\n",
        "if zoomfactor_1 != 1:\n",
        "  folder_option_2 = \"zoom\"\n",
        "  Saving_path = \"/\".join(Source_path.split(\"/\")[:-1])\n",
        "\n",
        "  aug_saving_path, split_img_folder_path,  log_path_file, multiplyer, nr_z_slices, nr_channels, nr_timepoints, x_dim, y_dim, x_div, y_div, use_RGB = img_split_pipeline(Source_path, Saving_path, divisor)\n",
        "\n",
        "  # create new folder for image separation\n",
        "  save_location = \"/\".join(split_img_folder_path.split(\"/\")[:-1])\n",
        "  folder_name = f\"{divisor}_img_separation\"\n",
        "  save_location = os.path.join(save_location,folder_name)\n",
        "  if not os.path.exists(save_location):\n",
        "    os.mkdir(save_location)\n",
        "  os.chdir(save_location)  \n",
        "\n",
        "  sub_save_location = data_preparation_for_zoominterpolation(folder_option_2, save_location, split_img_folder_path)\n",
        "  \n",
        "  img_folder_path_interpolate = prepare_files_for_zoominterpolation_step(sub_save_location, zoomfactor_1)\n",
        " \n",
        "  %reload_ext autoreload\n",
        "  os.chdir(\"/content/ZoomInterpolation/codes\")\n",
        "  !python test_new.py  --corr $blur_and_brightness_correction\n",
        "\n",
        "\n",
        "  today = datetime.now()\n",
        "  date_info = today.strftime('%Y%m%d_%H%M%S')\n",
        "  interpolate_location = \"/content/ZoomInterpolation/results/Custom_\"+date_info \n",
        "  os.rename(\"/content/ZoomInterpolation/results/Custom\", interpolate_location)\n",
        "\n",
        "  Source_path = save_interpolated_image(interpolate_location, Saving_path, log_path_file, divisor, zoomfactor_1, folder_option_2, use_RGB)\n",
        "  remove_folder.append(Source_path)\n",
        "\n",
        "\n",
        "if zoomfactor_2 != 1:\n",
        "  folder_option_2 = \"zoom\"\n",
        "  img_size = img_size*zoomfactor_1\n",
        "  if img_size<=512:\n",
        "    divisor = img_size\n",
        "  else: \n",
        "    divisor = 512\n",
        "  Saving_path = \"/\".join(Source_path.split(\"/\")[:-1])\n",
        "\n",
        "  aug_saving_path, split_img_folder_path,  log_path_file, multiplyer, nr_z_slices, nr_channels, nr_timepoints, x_dim, y_dim, x_div, y_div, use_RGB = img_split_pipeline(Source_path, Saving_path, divisor)\n",
        "\n",
        "  # create new folder for image separation\n",
        "  save_location = \"/\".join(split_img_folder_path.split(\"/\")[:-1])\n",
        "  folder_name = f\"{divisor}_img_separation\"\n",
        "  save_location = os.path.join(save_location,folder_name)\n",
        "  if not os.path.exists(save_location):\n",
        "    os.mkdir(save_location)\n",
        "  os.chdir(save_location)  \n",
        "\n",
        "  sub_save_location = data_preparation_for_zoominterpolation(folder_option_2, save_location, split_img_folder_path)\n",
        "  \n",
        "  img_folder_path_interpolate = prepare_files_for_zoominterpolation_step(sub_save_location, zoomfactor_2)\n",
        " \n",
        "  %reload_ext autoreload\n",
        "  os.chdir(\"/content/ZoomInterpolation/codes\")\n",
        "  !python test_new.py  --corr $blur_and_brightness_correction\n",
        "\n",
        "\n",
        "  today = datetime.now()\n",
        "  date_info = today.strftime('%Y%m%d_%H%M%S')\n",
        "  interpolate_location = \"/content/ZoomInterpolation/results/Custom_\"+date_info \n",
        "  os.rename(\"/content/ZoomInterpolation/results/Custom\", interpolate_location)\n",
        "\n",
        "  Source_path = save_interpolated_image(interpolate_location, Saving_path, log_path_file, divisor, zoomfactor_2, folder_option_2, use_RGB)\n",
        "  remove_folder.append(Source_path)\n",
        "\n",
        "\n",
        "#@markdown Provide a folder path on your gdrive to save the zoominterpolation result\n",
        "Save_path = \"/content/sample_data\" #@param {type:\"string\"}\n",
        "\n",
        "if not os.path.exists(Save_path):\n",
        "  os.mkdir(Save_path)\n",
        "import shutil\n",
        "\n",
        "folder_name = Source_path.split(\"/\")[-1]\n",
        "destination = os.path.join(Save_path, folder_name)\n",
        "shutil.copytree(Source_path, destination)\n",
        "\n",
        "# saves final result in the folder named \"final result\"\n",
        "# I created it in two folders to make the run from the notebook easier for the user at the beginning to be able to do the quality controle with a detemined folder name \"final_result\"\n",
        "final_date = Saving_path + f\"/{date_info}_final_result\"\n",
        "final_demo = Saving_path + \"/final_result\"\n",
        "if os.path.exists(final_demo):\n",
        "  shutil.rmtree(final_demo)\n",
        "shutil.copytree(Source_path, final_date)\n",
        "shutil.copytree(Source_path, final_demo)\n",
        "\n",
        "for path in remove_folder:\n",
        "  shutil.rmtree(path)\n",
        "\n",
        "#@markdown Result image sequence will be saved in a folder called `./final_result` and in the folder including the date at the beginning. \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tta4phe6VMtO"
      },
      "source": [
        "\n",
        "# **6 Error mapping and quality metrics estimation**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbeEauCL7YjG"
      },
      "source": [
        "**WORKFLOW**\n",
        "\n",
        "- You can compare the quality of first downsampled images (performed in section 3) and re-interpolated with the ground truth image used for down-scaling\n",
        "- The first cells loads the images and shows the dimensions\n",
        "- The second cell evaluates the image and saves 2 csv files showing the mean quality values and the other one showing the quality values for each slice. It also saves the SSIM and RMSE maps of the dataset in the same folder\n",
        "- The third cell shows you SSIM and RMSE maps of a selected image slice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_eedldMK8bs"
      },
      "source": [
        "#### Explanation of evaluation metrics\n",
        "<font size = 3>This option can be used to compare the downsampled-z or downsample-t or zoom option with the original file to estimate the performance of the network\n",
        "---\n",
        "\n",
        "<font size = 2>This section will display SSIM maps and RSE maps as well as calculating total SSIM, NRMSE and PSNR metrics for all the images provided in the \"GT_Image_QC\" and \"Prediction_Image_QC\" !\n",
        "\n",
        "<font size = 3>**1. The SSIM (structural similarity) map** \n",
        "\n",
        "<font size = 2>The SSIM metric is used to evaluate whether two images contain the same structures. It is a normalized metric and an SSIM of 1 indicates a perfect similarity between two images. Therefore for SSIM, the closer to 1, the better. The SSIM maps are constructed by calculating the SSIM metric in each pixel by considering the surrounding structural similarity in the neighbourhood of that pixel (currently defined as window of 11 pixels and with Gaussian weighting of 1.5 pixel standard deviation, see our Wiki for more info). \n",
        "\n",
        "<font size=2>**mSSIM** is the SSIM value calculated across the entire window of both images.\n",
        "\n",
        "<font size=2>**The output below shows the SSIM maps with the mSSIM**\n",
        "\n",
        "<font size = 3>**2. The RSE (Root Squared Error) map** \n",
        "\n",
        "<font size = 2>This is a display of the root of the squared difference between the normalized predicted and target or the source and the target. In this case, a smaller RSE is better. A perfect agreement between target and prediction will lead to an RSE map showing zeros everywhere (dark).\n",
        "\n",
        "\n",
        "<font size =2>**NRMSE (normalised root mean squared error)** gives the average difference between all pixels in the images compared to each other. Good agreement yields low NRMSE scores.\n",
        "\n",
        "<font size = 2>**PSNR (Peak signal-to-noise ratio)** is a metric that gives the difference between the ground truth and prediction (or source input) in decibels, using the peak pixel values of the prediction and the MSE between the images. The higher the score the better the agreement.\n",
        "\n",
        "<font size=2>**The output below shows the RSE maps with the NRMSE and PSNR values.**\n",
        "\n",
        "<font size=2>**Quality check just works for not RGB images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWGw4s-RK8bt",
        "cellView": "form"
      },
      "source": [
        "#@title ####**6.1 Check for image shift**\n",
        "#@markdown Select on sample predicted image and one GT image for comparison. This cell compares \n",
        "### Check for a channel shift of the reconstructed image\n",
        "import sys\n",
        "sys.path.insert(0,'/content/ZoomInterpolation/load_functions')\n",
        "from prepare_for_zoominterpolation import correct_channels\n",
        "from prepare_for_zoominterpolation import bcolors\n",
        "from quality_metrics_estimation import create_shift_image\n",
        "from quality_metrics_estimation import norm_minmse\n",
        "from quality_metrics_estimation import structural_similarity\n",
        "from quality_metrics_estimation import psnr\n",
        "from quality_metrics_estimation import get_full_file_paths\n",
        "from skimage import io\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# program a channel_shift\n",
        "Prediction_Image_QC = \"/content/ZoomInterpolation/demo/final_result\" #@param{type:\"string\"}\n",
        "GT_Image_QC = \"/content/ZoomInterpolation/demo/sim_particles_2DF_256\" #@param{type:\"string\"}\n",
        "\n",
        "# read out all images in the selected folder\n",
        "prediction_flist_paths = get_full_file_paths(Prediction_Image_QC)\n",
        "GT_flist_paths = get_full_file_paths(GT_Image_QC)\n",
        "prediction_flist_paths.sort()\n",
        "GT_flist_paths.sort()\n",
        "\n",
        "Predicted_image = prediction_flist_paths[-1]\n",
        "GT_Image = GT_flist_paths[-1]\n",
        "\n",
        "#@markdown You can either select a given image based on the t and z-slice. If unticked the center image of the z and z dimension will be selected automatically.\n",
        "select_slices = True #@param {type:\"boolean\"}\n",
        "\n",
        "int_img = io.imread(Predicted_image)\n",
        "int_img, use_RGB = correct_channels(int_img)\n",
        "print(int_img.shape)\n",
        "print(use_RGB)\n",
        "\n",
        "\n",
        "if select_slices:\n",
        "  t_slice = 7 #@param {type:\"raw\"}\n",
        "  z_slice = 0 #@param {type:\"raw\"}\n",
        "  #@markdown If multi channel image choose the channel of interest:\n",
        "  channel = 0 #@param {type:\"raw\"}\n",
        "else:\n",
        "  t_slice = t//2\n",
        "  z_slice = z//2\n",
        "\n",
        "# Check if the file is a RGB file\n",
        "if use_RGB:\n",
        "  t, z, y_dim, x_dim, c= int_img.shape\n",
        "  int_img = int_img[t_slice, z_slice,:,:,channel]\n",
        "else:\n",
        "  t, z, y_dim, x_dim = int_img.shape\n",
        "  int_img = int_img[t_slice, z_slice,:,:]\n",
        "\n",
        "# int_img = int_img[t_slice, z_slice,:,:]\n",
        "gt_img = io.imread(GT_Image)\n",
        "gt_img, use_RGB = correct_channels(gt_img)\n",
        "\n",
        "print(gt_img.shape)\n",
        "if use_RGB:\n",
        "  t, z, y_dim, x_dim, c= gt_img.shape\n",
        "  gt_img = gt_img[t_slice, z_slice,:,:,channel]\n",
        "\n",
        "else:\n",
        "  t, z, y_dim, x_dim = gt_img.shape\n",
        "  gt_img = gt_img[t_slice, z_slice,:,:]\n",
        "\n",
        "# try different shifts\n",
        "x_list = [-4,-3,-2,-1,0,1,2,3,4]\n",
        "y_list = [-4,-3,-2,-1,0,1,2,3,4]\n",
        "dict_NRMSE_values = {}\n",
        "dict_SSIM_values = {}\n",
        "dict_PSNR_values = {}\n",
        "\n",
        "for y_shift in y_list:\n",
        "  for x_shift in x_list:\n",
        "    # print(temp_img.shape)\n",
        "    shifted_img = create_shift_image(int_img, y_shift, x_shift)\n",
        "        \n",
        "    # evaluate SSIM, PSNR, NRMSE\n",
        "    test_GT_norm, test_prediction_norm = norm_minmse(gt_img, shifted_img, normalize_gt=True)\n",
        "    img_RSE_GTvsPrediction = np.sqrt(np.square(test_GT_norm - test_prediction_norm))\n",
        "    NRMSE_GTvsPrediction = np.sqrt(np.mean(img_RSE_GTvsPrediction))\n",
        "    index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = structural_similarity(test_GT_norm, test_prediction_norm, data_range=1.0, full=True)\n",
        "    PSNR_GTvsPrediction = psnr(test_GT_norm, test_prediction_norm, data_range=1.0)\n",
        "\n",
        "\n",
        "    dict_NRMSE_values[str(y_shift)+ \"_\" +str(x_shift)] = [NRMSE_GTvsPrediction]\n",
        "    dict_SSIM_values[str(y_shift)+ \"_\" +str(x_shift)] = [index_SSIM_GTvsPrediction]\n",
        "    dict_PSNR_values[str(y_shift)+ \"_\" +str(x_shift)] = [PSNR_GTvsPrediction]\n",
        "\n",
        "    \n",
        "    # print(f\"y_shift {y_shift}, x_shift  {x_shift}\")\n",
        "    # print(f\"NRMSE_GTvsPrediction {NRMSE_GTvsPrediction}, index_SSIM_GTvsPrediction {index_SSIM_GTvsPrediction}, PSNR_GTvsPrediction {PSNR_GTvsPrediction}\")\n",
        "\n",
        "\n",
        "## uncommented - COMMENT: removed it because confusing for the user. just useful if for some reason the image gets shifted for some pixels in the x or y direction. then this could be uncommented to find the number of shifted pixels and correct it accordingly.\n",
        "\n",
        "# import pylab as pl\n",
        "# # pixel_shift evaluation\n",
        "# pd.DataFrame.from_dict(dict_SSIM_values).transpose().plot.bar(figsize=(25,2))\n",
        "# print(f\"best conditions according to SSIM: {max(dict_SSIM_values, key=dict_SSIM_values.get)}\")\n",
        "# pl.suptitle(\"Different values for SSIM accoring to different x-y shifts - max best\")\n",
        "\n",
        "\n",
        "# pd.DataFrame.from_dict(dict_PSNR_values).transpose().plot.bar(figsize=(25,2))\n",
        "# print(f\"best conditions according to PSNR: {max(dict_PSNR_values, key=dict_PSNR_values.get)}\")\n",
        "# pl.suptitle(\"Different values for PSNR accoring to different x-y shifts - max best\")\n",
        "\n",
        "# pd.DataFrame.from_dict(dict_NRMSE_values).transpose().plot.bar(figsize=(25,2))\n",
        "# print(f\"best conditions according to NRMSE: {min(dict_NRMSE_values, key=dict_NRMSE_values.get)}\")\n",
        "# pl.suptitle(\"Different values for PSNR accoring to different y-x shifts - min best\")\n",
        "\n",
        "\n",
        "\n",
        "#################################################################\n",
        "#@title ####**4.3.3 Perform quality control over all files**\n",
        "#@markdown Choose the folders to compare original with predicted images with the ground truth images for quality control\n",
        "from prepare_for_zoominterpolation import  make_folder_with_date\n",
        "from quality_metrics_estimation import get_full_file_paths\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Prediction_Image_QC = \"/content/gdrive/MyDrive/1.2_BIG_DATA_PhD_Project_2/0.PAPER_EXPERIMENT/20210321_Tchern_dataset_Usecase/Zoomed_512_T/128_13_BIC_2x\" #@param{type:\"string\"}\n",
        "# GT_Image_QC = \"/content/gdrive/MyDrive/1.2_BIG_DATA_PhD_Project_2/0.PAPER_EXPERIMENT/20210321_Tchern_dataset_Usecase/Zoomed_512_T/256_13\" #@param{type:\"string\"}\n",
        "\n",
        "# Create quality control folder\n",
        "base_folder = \"/\".join(Prediction_Image_QC.split(\"/\")[:-1])\n",
        "exp_name = Prediction_Image_QC.split(\"/\")[-1]\n",
        "quality_test_folder = os.path.join(base_folder, f\"quality_tests-{exp_name}\")\n",
        "\n",
        "print(quality_test_folder)\n",
        "if os.path.exists(quality_test_folder)==False:\n",
        "  os.mkdir(quality_test_folder)\n",
        "# quality_test_folder = make_folder_with_date(base_folder, f\"{exp_name}_quality_tests\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if len(prediction_flist_paths) != len(GT_flist_paths):\n",
        "  print(f\"{bcolors.WARNING}There is an unequal number of images in the two selected folders{bcolors.ENDC}\")\n",
        "else:\n",
        "  print(f\"There are {len(GT_flist_paths)} files in both folders\")\n",
        "\n",
        "file_num = len(GT_flist_paths)\n",
        "\n",
        "for num in range(file_num):\n",
        "    # create log-csv files for the mean of each stack and the similarity/error within each image slice\n",
        "    with open(quality_test_folder +f'/{exp_name}_every_slice{num}.csv', 'w', newline='') as file:\n",
        "          writer = csv.writer(file)\n",
        "          writer.writerow([\"File\", \"Z-Dimension\", \"T-Dimension\", \"index_SSIM_GTvsPrediction\", \"NRMSE_GTvsPrediction\",\"PSNR_GTvsPrediction\"])\n",
        "\n",
        "    with open(quality_test_folder +f'/{exp_name}_mean{num}.csv', 'w', newline='') as file:\n",
        "          writer = csv.writer(file)\n",
        "          writer.writerow([\"File\", \"mSSIM_GvP_mean\", \"NRMSE_GvP_mean\", \"PSNR_GvP_mean\"])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUKo-Bna4fFF",
        "cellView": "form"
      },
      "source": [
        "#@title ####**6.2 Run quality control**\n",
        "\n",
        "#@markdown This cells saves the evaluation images and values for SSIM, PSNR, MSR in \"*.csv\" files.\n",
        "#@markdown You can either select a manual shift for the x and y-dimension of the images. If unticked the best conditions calculated in **Section 6.1** will be take.\n",
        "\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "select_manual_shift = True\n",
        "y_shift = 0\n",
        "x_shift = 0\n",
        "#@markdown If multi channel image choose the channel of interest:\n",
        "channel = 0 #@param {type:\"raw\"}\n",
        "\n",
        "## uncommented - COMMENT: removed it because confusing for the user. just useful if for some reason the image gets shifted for some pixels in the x or y direction. then this could be uncommented to find the number of shifted pixels and correct it accordingly.\n",
        "# if select_manual_shift:\n",
        "#   y_shift = 0 #@param {type:\"raw\"}\n",
        "#   x_shift =  0#@param {type:\"raw\"}\n",
        "#   #@markdown If multi channel image choose the channel of interest:\n",
        "#   channel = 0 #@param {type:\"raw\"}\n",
        "# else:\n",
        "#   y_shift = int(max(dict_PSNR_values, key=dict_PSNR_values.get).split(\"_\")[0])\n",
        "#   x_shift = int(max(dict_PSNR_values, key=dict_PSNR_values.get).split(\"_\")[1])\n",
        "#   channel = 0 \n",
        "\n",
        "# These lists will be used to collect all the metrics values per slice\n",
        "file_name_list = []\n",
        "T_slice_number_list = []\n",
        "Z_slice_number_list = []\n",
        "mSSIM_GvP_list = []\n",
        "NRMSE_GvP_list = []\n",
        "PSNR_GvP_list = []\n",
        "\n",
        "\n",
        "img_GT = io.imread(GT_flist_paths[0])\n",
        "img_GT, use_RGB = correct_channels(img_GT)\n",
        "\n",
        "img_prediction = io.imread(prediction_flist_paths[0])\n",
        "img_prediction, use_RGB = correct_channels(img_prediction)\n",
        "\n",
        " \n",
        "# img_prediction = img_prediction.get_image_data(\"TZXY\")\n",
        "\n",
        "t_slices = img_prediction.shape[0]\n",
        "z_slices = img_prediction.shape[1]\n",
        "\n",
        "\n",
        "# here we analyse just the channel zero. If other channels are of interest please change the number in the sliceing. \n",
        "# if the number of frames gets shifted by 1, here we slice the image to the smaller size\n",
        "if use_RGB:\n",
        "  img_prediction = img_prediction[:t_slices,:z_slices,:,:,channel]\n",
        "  img_GT = img_GT[:t_slices,:z_slices,:,:,channel]\n",
        "\n",
        "print(img_GT.shape)\n",
        "print(img_prediction.shape)\n",
        "print(f\"The t-dimension is {t_slices}, and the z-dimension is {z_slices}\")\n",
        "\n",
        "#  Create a zero matrix as template for the images\n",
        "img_SSIM_GTvsPrediction_stack = np.zeros((t_slices, z_slices, img_prediction.shape[2], img_prediction.shape[3]))\n",
        "img_RSE_GTvsPrediction_stack = np.zeros((t_slices, z_slices, img_prediction.shape[2], img_prediction.shape[3]))\n",
        "\n",
        "# compare each image - slice per slice and save the calculation in csvs and as new image\n",
        "\n",
        "num = 0 \n",
        "for pred_file_path, GT_file_path in zip(prediction_flist_paths, GT_flist_paths):\n",
        "    # load the two images\n",
        "    img_prediction = io.imread(pred_file_path)\n",
        "    img_prediction, use_RGB = correct_channels(img_prediction)\n",
        "\n",
        "    # img_prediction = img_prediction.get_image_data(\"TZXY\")\n",
        "    img_GT = io.imread(GT_file_path)\n",
        "    img_GT, use_RGB = correct_channels(img_GT)\n",
        "\n",
        "    if use_RGB:\n",
        "      img_prediction = img_prediction[:t_slices,:z_slices,:,:,channel]\n",
        "      img_GT = img_GT[:t_slices,:z_slices,:,:,channel]\n",
        "\n",
        "    # img_GT = img_GT.get_image_data(\"TZXY\")\n",
        "    file_name = \"/\".join(GT_file_path.split(\"/\")[-1:])\n",
        "    for z in tqdm(range(z_slices)): \n",
        "        for t in range(t_slices): \n",
        "\n",
        "          # --------------------------- perform optimal shift---------------------------\n",
        "          \n",
        "          one_img_prediction = img_prediction[t][z]\n",
        "          one_img_prediction = create_shift_image(one_img_prediction, y_shift, x_shift)\n",
        "\n",
        "          # -------------------------------- Prediction --------------------------------\n",
        "\n",
        "          test_GT_norm, test_prediction_norm = norm_minmse(img_GT[t][z], one_img_prediction, normalize_gt=True)\n",
        "\n",
        "          # ------------------------ Calculate the SSIM metric and maps ----------------\n",
        "          # Calculate the SSIM maps and index\n",
        "          index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = structural_similarity(test_GT_norm, test_prediction_norm, data_range=1.0, full=True)\n",
        "\n",
        "          #Calculate ssim_maps\n",
        "          # img_SSIM_GTvsPrediction_stack[t][z] = np.float32(img_SSIM_GTvsPrediction)\n",
        "          img_SSIM_GTvsPrediction_stack[t][z] = img_SSIM_GTvsPrediction\n",
        "\n",
        "\n",
        "          # ----------------------- Calculate the NRMSE metrics ------------------------\n",
        "          # Calculate the Root Squared Error (RSE) maps\n",
        "          img_RSE_GTvsPrediction = np.sqrt(np.square(test_GT_norm - test_prediction_norm))\n",
        "\n",
        "          # Calculate SE maps\n",
        "          img_RSE_GTvsPrediction_stack[t][z] = np.float32(img_RSE_GTvsPrediction)\n",
        "\n",
        "\n",
        "          # Normalised Root Mean Squared Error (here it's valid to take the mean of the image)\n",
        "          NRMSE_GTvsPrediction = np.sqrt(np.mean(img_RSE_GTvsPrediction))\n",
        "\n",
        "          # Calculate the PSNR between the images\n",
        "          PSNR_GTvsPrediction = psnr(test_GT_norm, test_prediction_norm, data_range=1.0)\n",
        "\n",
        "          with open(quality_test_folder +f'/{exp_name}_every_slice{num}.csv', 'a', newline='') as file:\n",
        "              writer = csv.writer(file)\n",
        "              writer.writerow([file_name, z, t, index_SSIM_GTvsPrediction, NRMSE_GTvsPrediction,PSNR_GTvsPrediction])\n",
        "\n",
        "          mSSIM_GvP_list.append(index_SSIM_GTvsPrediction)\n",
        "          NRMSE_GvP_list.append(NRMSE_GTvsPrediction)\n",
        "          PSNR_GvP_list.append(PSNR_GTvsPrediction)\n",
        "\n",
        "    # ----------- Change the stacks to 32 bit images and fix channels -----------\n",
        "    from skimage import img_as_float32\n",
        "    img_SSIM_GTvsPrediction_stack_32 = img_as_float32(img_SSIM_GTvsPrediction_stack, force_copy=False)\n",
        "    # img_SSIM_GTvsPrediction_stack_32= reshape_data(img_SSIM_GTvsPrediction_stack_32, \"TZXY\",\"STCZXY\")\n",
        "\n",
        "    img_RSE_GTvsPrediction_stack_32 = img_as_float32(img_RSE_GTvsPrediction_stack, force_copy=False)\n",
        "    # img_RSE_GTvsPrediction_stack_32= reshape_data(img_RSE_GTvsPrediction_stack_32, \"TZXY\",\"STCZXY\")\n",
        "\n",
        "\n",
        "    # ----------- Saving the error map stacks -----------\n",
        "    path_file = os.path.join(quality_test_folder, file_name)\n",
        "    SSIM_path = path_file[:-4] + \"SSIM_GTvsPrediction.tif\"\n",
        "    RSE_path =  path_file[:-4] + 'RSE_GTvsPrediction.tif'\n",
        "\n",
        "    io.imsave(SSIM_path,img_SSIM_GTvsPrediction_stack_32)\n",
        "    io.imsave(RSE_path,img_RSE_GTvsPrediction_stack_32)\n",
        "\n",
        "    # ----------- Saving mean errors in CSV -----------\n",
        "    mSSIM_GvP_mean = (sum(mSSIM_GvP_list)/len(mSSIM_GvP_list))\n",
        "    NRMSE_GvP_mean = (sum(NRMSE_GvP_list)/len(NRMSE_GvP_list))\n",
        "    PSNR_GvP_mean = (sum(PSNR_GvP_list)/len(PSNR_GvP_list))\n",
        "    with open(quality_test_folder + f'/{exp_name}_mean{num}.csv', 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([file_name, mSSIM_GvP_mean, NRMSE_GvP_mean, PSNR_GvP_mean])\n",
        "    num += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK2iuVKl4iXV",
        "cellView": "form"
      },
      "source": [
        "#@title ####**6.3 Visualize single images from a selected file**\n",
        "\n",
        "#@markdown Select a file number (based on the number of image that were processed before) and the t/z slices of images to compare\n",
        "#@markdown (Index starts with 0)\n",
        "import matplotlib.pyplot as plt\n",
        "from prepare_for_zoominterpolation import bcolors\n",
        "\n",
        "# nr_file =  0#@param {type:\"integer\"}\n",
        "z_dimension =  0#@param {type:\"integer\"}\n",
        "t_dimension =  3#@param {type:\"integer\"}\n",
        "\n",
        "\n",
        "select_manual_shift = True\n",
        "y_shift = 0\n",
        "x_shift = 0\n",
        "#@markdown If multi channel image choose the channel of interest:\n",
        "channel = 0 #@param {type:\"raw\"}\n",
        "\n",
        "\n",
        "## uncommented - COMMENT: removed it because confusing for the user. just useful if for some reason the image gets shifted for some pixels in the x or y direction. then this could be uncommented to find the number of shifted pixels and correct it accordingly.\n",
        "\n",
        "# #@markdown You can either select a manual shift for the x and y-dimension of the images. If unticked the best conditions ecaluated in **Section 3.4.2** will be take.\n",
        "# select_manual_shift = True #@param {type:\"boolean\"}\n",
        "# if select_manual_shift:\n",
        "#   y_shift = 0 #@param {type:\"raw\"}\n",
        "#   x_shift = 0 #@param {type:\"raw\"}\n",
        "#   #@markdown If multi channel image choose the channel of interest:\n",
        "#   channel =  0#@param {type:\"raw\"}\n",
        "# else:\n",
        "#   y_shift = int(max(dict_PSNR_values, key=dict_PSNR_values.get).split(\"_\")[0])\n",
        "#   x_shift = int(max(dict_PSNR_values, key=dict_PSNR_values.get).split(\"_\")[1])\n",
        "#   channel = 0 \n",
        "\n",
        "display_all_files =True\n",
        "if display_all_files== True:\n",
        "   for nr_file in range(file_num):\n",
        "        Prediction_Image_QC = prediction_flist_paths[nr_file]\n",
        "        GT_Image_QC = GT_flist_paths[nr_file]\n",
        "\n",
        "        # get file names of both\n",
        "        file_name_pred = \"/\".join(Prediction_Image_QC.split(\"/\")[-1:])\n",
        "        file_name_GT = \"/\".join(GT_Image_QC.split(\"/\")[-1:])\n",
        "\n",
        "        # load the two images\n",
        "        img_prediction = io.imread(Prediction_Image_QC)\n",
        "        img_prediction, use_RGB = correct_channels(img_prediction)\n",
        "\n",
        "        # img_prediction = img_prediction.get_image_data(\"TZXY\")\n",
        "        img_GT = io.imread(GT_Image_QC)\n",
        "        img_GT, use_RGB = correct_channels(img_GT)\n",
        "        \n",
        "        # correct for RGB and uneven slice number\n",
        "        if use_RGB:\n",
        "          img_prediction = img_prediction[:t_slices,:z_slices,:,:,channel]\n",
        "          img_GT = img_GT[:t_slices,:z_slices,:,:,channel]\n",
        "        # img_GT = img_GT.get_image_data(\"TZXY\")\n",
        "\n",
        "        prediction_dim =img_prediction.shape\n",
        "        print(f\"Source dimension: {prediction_dim}\")\n",
        "        GT_dim =img_GT.shape\n",
        "        print(f\"Target dimension: {GT_dim}\")\n",
        "\n",
        "        #prepare slice\n",
        "        prediction_img_slice = img_prediction[t_dimension,z_dimension,:,:]\n",
        "        GT_img_slice = img_GT[t_dimension,z_dimension,:,:]\n",
        "        print(GT_img_slice.shape)\n",
        "\n",
        "\n",
        "        # -------------------------------- Calculate the SSIM metric and maps --------------------------------\n",
        "\n",
        "        img_GT_slice = img_GT[t_dimension][z_dimension]\n",
        "        img_prediction_slice = img_prediction[t_dimension][z_dimension]\n",
        "        img_prediction_slice = create_shift_image(img_prediction_slice, y_shift, x_shift)\n",
        "\n",
        "        test_GT_norm, test_prediction_norm = norm_minmse(img_GT_slice, img_prediction_slice, normalize_gt=True)\n",
        "\n",
        "        # Calculate the SSIM maps and index data_range=test_prediction_norm.max() - test_prediction_norm.min()\n",
        "        index_SSIM_GTvsPrediction, img_SSIM_GTvsPrediction = structural_similarity(test_GT_norm, test_prediction_norm, data_range=1, full=True)\n",
        "        img_SSIM_GTvsPrediction = np.float32(img_SSIM_GTvsPrediction)\n",
        "\n",
        "        # Calculate the RSE\n",
        "        img_RSE_GTvsPrediction = np.sqrt(np.square(test_GT_norm - test_prediction_norm))\n",
        "        img_RSE_GTvsPrediction = np.float32(img_RSE_GTvsPrediction)\n",
        "\n",
        "        # Normalised Root Mean Squared Error (here it's valid to take the mean of the image)\n",
        "        NRMSE_GTvsPrediction = np.sqrt(np.mean(img_RSE_GTvsPrediction))\n",
        "\n",
        "        # Calculate the PSNR between the images\n",
        "        PSNR_GTvsPrediction = psnr(test_GT_norm, test_prediction_norm, data_range=1.0)\n",
        "\n",
        "\n",
        "        # -------------------------------- Plotting --------------------------------\n",
        "\n",
        "        plt.figure(figsize=(15,15))\n",
        "        # Currently only displays the last computed set, from memory\n",
        "\n",
        "        #Setting up colours\n",
        "        cmap = plt.cm.Greys\n",
        "        cmap = cmap.reversed()\n",
        "\n",
        "        # Target (Ground-truth)\n",
        "        plt.subplot(2,3,1)\n",
        "        plt.tick_params(\n",
        "            axis='both',      # changes apply to the x-axis and y-axis\n",
        "            which='both',      # both major and minor ticks are affected\n",
        "            bottom=False,      # ticks along the bottom edge are off\n",
        "            top=False,        # ticks along the top edge are off\n",
        "            left=False,       # ticks along the left edge are off\n",
        "            right=False,         # ticks along the right edge are off\n",
        "            labelbottom=False,\n",
        "            labelleft=False)  \n",
        "        plt.xlabel(f'image {file_name}')\n",
        "        plt.imshow(img_GT_slice, cmap = cmap)\n",
        "        plt.title('GT (slice #'+str(t_dimension)+'-'+str(t_dimension)+')')\n",
        "        \n",
        "\n",
        "        # Source\n",
        "        plt.subplot(2,3,2)\n",
        "        plt.tick_params(\n",
        "            axis='both',      # changes apply to the x-axis and y-axis\n",
        "            which='both',      # both major and minor ticks are affected\n",
        "            bottom=False,      # ticks along the bottom edge are off\n",
        "            top=False,        # ticks along the top edge are off\n",
        "            left=False,       # ticks along the left edge are off\n",
        "            right=False,         # ticks along the right edge are off\n",
        "            labelbottom=False,\n",
        "            labelleft=False)  \n",
        "        plt.xlabel(f'image {file_name}')\n",
        "        plt.imshow(img_prediction_slice, aspect='equal', cmap = cmap)\n",
        "        plt.title('Prediction (slice #'+str(t_dimension)+'-'+str(t_dimension)+')')\n",
        "\n",
        "\n",
        "        #Setting up colours\n",
        "        plt.figure(figsize=(15,15))\n",
        "        cmap = plt.cm.CMRmap\n",
        "\n",
        "        # img_SSIM_GTvsPrediction\n",
        "        plt.subplot(2,3,1)\n",
        "        plt.tick_params(\n",
        "            axis='both',      # changes apply to the x-axis and y-axis\n",
        "            which='both',      # both major and minor ticks are affected\n",
        "            bottom=False,      # ticks along the bottom edge are off\n",
        "            top=False,        # ticks along the top edge are off\n",
        "            left=False,       # ticks along the left edge are off\n",
        "            right=False,         # ticks along the right edge are off\n",
        "            labelbottom=False,\n",
        "            labelleft=False)  \n",
        "        plt.xlabel(f'image {file_name}')\n",
        "        img_SSIM_GTvsPrediction_plt = plt.imshow(img_SSIM_GTvsPrediction, cmap = cmap, vmin=0,vmax=1)\n",
        "        plt.colorbar(img_SSIM_GTvsPrediction_plt,fraction=0.046, pad=0.04)\n",
        "        plt.title('iSSIM map: GT vs. Prediction (slice #'+str(t_dimension)+'-'+str(t_dimension)+')')\n",
        "\n",
        "        # img_RSE_GTvsPrediction\n",
        "        plt.subplot(2,3,2)\n",
        "        plt.tick_params(\n",
        "            axis='both',      # changes apply to the x-axis and y-axis\n",
        "            which='both',      # both major and minor ticks are affected\n",
        "            bottom=False,      # ticks along the bottom edge are off\n",
        "            top=False,        # ticks along the top edge are off\n",
        "            left=False,       # ticks along the left edge are off\n",
        "            right=False,         # ticks along the right edge are off\n",
        "            labelbottom=False,\n",
        "            labelleft=False)  \n",
        "        plt.xlabel(f'image {file_name}')\n",
        "        RSE_GTvsPrediction_plt = plt.imshow(img_RSE_GTvsPrediction, aspect='equal', cmap = cmap, vmin=0,vmax=1)\n",
        "        plt.colorbar(RSE_GTvsPrediction_plt,fraction=0.046, pad=0.04)\n",
        "        plt.title('img_RSE_GTvsPrediction (slice #'+str(t_dimension)+'-'+str(t_dimension)+')')\n",
        "\n",
        "        print(f\"{bcolors.WARNING}PSNR_GTvsPrediction:       {PSNR_GTvsPrediction}{bcolors.ENDC}\")\n",
        "        print(f\"{bcolors.WARNING}NRMSE_GTvsPrediction:      {NRMSE_GTvsPrediction}{bcolors.OKBLUE}\")\n",
        "        print(f\"{bcolors.WARNING}index_SSIM_GTvsPrediction: {index_SSIM_GTvsPrediction}{bcolors.OKGREEN}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24b8nbCYSAhT"
      },
      "source": [
        "# Good Luck!"
      ]
    }
  ]
}
